{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Recommend Movies or Shows to Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from:\n",
    "- [Implementing a Recommender System with SageMaker, MXNet, and Gluon](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/gluon_recommender_system/gluon_recommender_system.ipynb)\n",
    "- [An Introduction to Factorization Machines with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/factorization_machines_mnist/factorization_machines_mnist.ipynb)\n",
    "- [Extending Amazon SageMaker Factorization Machines Algorithm to Predict Top X Recommendations](https://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to business scenario\n",
    "\n",
    "You work for a startup that focuses on delivering on-demand video streaming services to users. The company wants to introduce movie/show recommendations for their users based on their viewing history.\n",
    "\n",
    "You are tasked with solving part of this problem by leveraging machine learning to create a recommendation engine to be used on the user website. You are given access to the dataset of historical user preferences and the movies they watched. You can use this to train a machine learning model to recommend movies/shows to watch.\n",
    "\n",
    "## About this dataset  \n",
    "The Amazon Customer Reviews Dataset is a collection of reviews on different products from the Amazon.com marketplace from 1995 until 2015. Customer reviews are one of the most important data types at Amazon. Collecting and showing reviews has been part of the Amazon culture since the beginning of the company and is arguably one important source of innovation. For more details on this dataset, see [Amazon Customer Reviews Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "This exercise focuses on reviews of videos. The videos dataset contains 1- to 5-star ratings from over 2M Amazon customers on 160K digital videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "**Data columns**\n",
    "\n",
    "- `marketplace`: Two-letter country code (in this case, all \"US\")\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author\n",
    "- `review_id`: Unique ID for the review\n",
    "- `product_id`: Amazon Standard Identification Number (ASIN). http://www.amazon.com/dp/<ASIN\\> links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN. Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case, digital videos)\n",
    "- `star_rating`: Product's rating (1 to 5 stars)\n",
    "- `helpful_votes`: Number of helpful votes for the review\n",
    "- `total_votes`: Number of total votes the review received\n",
    "- `vine`: Was the review written as part of the Vine program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: Title of the review itself\n",
    "- `review_body`: Text of the review\n",
    "- `review_date`: Date the review was written\n",
    "\n",
    "\n",
    "**Data format**\n",
    "- Tab `\\t` separated text file, without quote or escape characters\n",
    "- First line in each file is header; 1 line corresponds to 1 record\n",
    "\n",
    "### Dataset attributions\n",
    "\n",
    "Website: https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "This dataset is being provided to you by permission of Amazon and is subject to the terms of the AWS Digital Training Service Agreement (available at https://aws.amazon.com/training/digital-training-agreement). You are expressly prohibited from copying, modifying, selling, exporting, or using this dataset in any way other than for the purpose of completing this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorming and designing a question...\n",
    "\n",
    "...That you can answer with machine learning. \n",
    "\n",
    "The first step in most projects is to think about the question you want to ask, how the data available supports this question, and which tool (in this case, machine learning model) you are going to use to answer the question. This is an important step because it helps narrow the scope of exploration and gives clarity on the features that you are going to use. \n",
    "\n",
    "Take a moment to write your thoughts regarding the dataset in the cell below. What are the things you can predict with machine learning? Why may that be relevant from a business/client perspective? Explain why you consider these thoughts important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas\n",
    "\n",
    "- Recommend movies to user's based on other user's likings, example: Neflix ALS algorithm\n",
    "- Predict a user review to a movie they haven't seen yet\n",
    "- Can we use sentiment analysis on the Review for a little more information about what the user tought about the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be several ideas about what to do with the data, but for now we are all going to work on recommending a video to a particular user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation and factorization machines\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning. One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature. The million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and machine learning algorithms. Most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework. However, the core component is almost always a model that predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users. The minimal required dataset for this is a history of user item ratings (which we have).\n",
    "\n",
    "The method that you'll use is a factorization machine. A factorization machine is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model and is designed to parsimoniously (simply) capture interactions between features in high-dimensional sparse datasets. This makes it a good candidate to handle data patterns with features such as click prediction and item recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Problem formulation and data collection\n",
    "\n",
    "Start this project off by writing a few sentences below that summarize the business problem and the business goal you're trying to achieve in this scenario. Include a business metric you would like your team to aspire toward. With that information defined, clearly write out the machine learning problem statement. Finally, add a comment or two about the type of machine learning this represents.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Include a summary of these details in your project presentations.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read through a business scenario and:\n",
    "\n",
    "### 1. Determine if and why ML is an appropriate solution to deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML is appropriate, as we have a large dataset of products and users to work with, doing a manual job for this would be an infinite task, and cost a lot more than simply employing machines for what they do best, detecting patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formulate the business problem, success metrics, and desired ML output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a company, we need to increase user retention in the platform, by providing them with useful recommendations, that will drive them to watch more content in the platform.\n",
    "- A success metric would be measuring the average watch time, and mesure if it has improved retention.\n",
    "- Another way to measure success would be the click through rate on recommendations, and once they are clicked, how many of them are then watched by the user it was recommended to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify the type of ML problem you’re dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is an Association rule learning problem, where the model will try to predict associated movies for a user, given the history of other movies that user as watched and rated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze the appropriateness of the data you’re working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data has all the necessary information available, there are some extraneous information that may be removed as it does not relate to the problem at hand (e.g.: marketplace, review_id, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Now that we have decided where to focus our energy, let's set things up so you can start working on solving the problem.\n",
    "\n",
    "**Note:** This notebook was created and tested on an `ml.m4.xlarge` notebook instance. \n",
    "\n",
    "Start by specifying:\n",
    "- The Amazon Simple Storage Service (Amazon S3) bucket and prefix(?) that you want to use for training and model data. This should be within the same Region as the Notebook Instance, training, and hosting.\n",
    "- The AWS Identity and Access Management (IAM) role [Amazon Resource Name (ARN)](https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html) used to give training and hosting access to your data. See the documentation for how to create these.\n",
    "\n",
    "**Note:** If more than one role is required for notebook instances, training, and/or hosting, replace the `get_execution_role()` call with the appropriate full IAM role ARN string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace **`<LabBucketName>`** with the resource name that was provided with your lab account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the bucket and prefix according to your information\n",
    "bucket = 'qls-4019912-5ead0778b062168c-labbucket-1h2g0kyyebu6d'\n",
    "prefix = 'sagemaker-fm' \n",
    "\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load some Python libraries you'll need for the remainder of this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add this to display all the outputs in the cell and not just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data preprocessing and visualization \n",
    "In this data preprocessing phase, you should take the opportunity to explore and visualize your data to better understand it. First, import the necessary libraries and read the data into a Pandas dataframe. After that, explore your data. Look for the shape of the dataset and explore your columns and the types of columns you're working with (numerical, categorical). Consider performing basic statistics on the features to get a sense of feature means and ranges. Take a close look at your target column and determine its distribution.\n",
    "\n",
    "### Specific questions to consider\n",
    "1. What can you deduce from the basic statistics you ran on the features? \n",
    "\n",
    "2. What can you deduce from the distributions of the target classes?\n",
    "\n",
    "3. Is there anything else you deduced from exploring the data?\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Include a summary of your answers to these and other similar questions in your project presentations.</span>\n",
    "\n",
    "Start by bringing in the dataset from an Amazon S3 public bucket to this notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Check whether the file is already in the desired path or if it needs to be downloaded\n",
    "\n",
    "base_path = '/home/ec2-user/SageMaker/project/data/AmazonReviews'\n",
    "file_path = '/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz'\n",
    "\n",
    "if not os.path.isfile(base_path + file_path):\n",
    "    subprocess.run(['mkdir', '-p', base_path])\n",
    "    subprocess.run(['aws', 's3', 'cp', 's3://amazon-reviews-pds/tsv' + file_path, base_path])\n",
    "else:\n",
    "    print('File already downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "Read the data into a Pandas dataframe so that you can know what you are dealing with.\n",
    "\n",
    "**Note:** You'll set `error_bad_lines=False` when reading the file in, because there appear to be a very small number of records that would create a problem otherwise.\n",
    "\n",
    "**Hint:** You can use the built-in Python `read_csv` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)). You can use the file path directly with Pandas `read_csv` with `delimiter='\\t'`.\n",
    "\n",
    "For example: `pd.read_csv('filename.tar.gz', delimiter = '\\t', error_bad_lines=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/'.join([base_path, file_path]), delimiter = '\\t', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first few rows of your dataset.  \n",
    "\n",
    "**Hint**: Use the `pandas.head(<number>)` function to print the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12190288</td>\n",
       "      <td>R3FU16928EP5TC</td>\n",
       "      <td>B00AYB1482</td>\n",
       "      <td>668895143</td>\n",
       "      <td>Enlightened: Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I loved it and I wish there was a season 3</td>\n",
       "      <td>I loved it and I wish there was a season 3... ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>30549954</td>\n",
       "      <td>R1IZHHS1MH3AQ4</td>\n",
       "      <td>B00KQD28OM</td>\n",
       "      <td>246219280</td>\n",
       "      <td>Vicious</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>52895410</td>\n",
       "      <td>R52R85WC6TIAH</td>\n",
       "      <td>B01489L5LQ</td>\n",
       "      <td>534732318</td>\n",
       "      <td>After Words</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Charming movie</td>\n",
       "      <td>This movie isn't perfect, but it gets a lot of...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>27072354</td>\n",
       "      <td>R7HOOYTVIB0DS</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>239012694</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>excellant this is what tv should be</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26939022</td>\n",
       "      <td>R1XQ2N5CDOZGNX</td>\n",
       "      <td>B0094LZMT0</td>\n",
       "      <td>535858974</td>\n",
       "      <td>On The Waterfront</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Brilliant film from beginning to end</td>\n",
       "      <td>Brilliant film from beginning to end. All of t...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>4772040</td>\n",
       "      <td>R1HCST57W334KN</td>\n",
       "      <td>B0112OSOQE</td>\n",
       "      <td>38517795</td>\n",
       "      <td>Rick and Morty Season 2</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Best show on TV right now</td>\n",
       "      <td>If you don't like this show. Go back to your n...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US</td>\n",
       "      <td>12910040</td>\n",
       "      <td>R32BUTYQS1ZJBQ</td>\n",
       "      <td>B000NPE5SA</td>\n",
       "      <td>373323715</td>\n",
       "      <td>Africa Screams</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Very funny. A typical mid 50's comedy</td>\n",
       "      <td>Very funny.  A typical mid 50's comedy.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>US</td>\n",
       "      <td>38805573</td>\n",
       "      <td>RH4SXPL4L9QU</td>\n",
       "      <td>B00XWV4QXG</td>\n",
       "      <td>633842417</td>\n",
       "      <td>Entourage: Season 7</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>it was not as good as the series</td>\n",
       "      <td>Strange as it is, it was not as good as the se...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>37100714</td>\n",
       "      <td>R37INWIQA5YW8N</td>\n",
       "      <td>B00X8UKOUK</td>\n",
       "      <td>666093513</td>\n",
       "      <td>Catastrophe - Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>Funny shows! We laughed out loud, alot!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>US</td>\n",
       "      <td>41234409</td>\n",
       "      <td>R18GSVAAS3N8GX</td>\n",
       "      <td>B00OOKXTFU</td>\n",
       "      <td>801680808</td>\n",
       "      <td>The Worricker Trilogy Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>A tad Stuffy</td>\n",
       "      <td>Well made. Great actors! Christopher Walken ma...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12190288  R3FU16928EP5TC  B00AYB1482       668895143   \n",
       "1          US     30549954  R1IZHHS1MH3AQ4  B00KQD28OM       246219280   \n",
       "2          US     52895410   R52R85WC6TIAH  B01489L5LQ       534732318   \n",
       "3          US     27072354   R7HOOYTVIB0DS  B008LOVIIK       239012694   \n",
       "4          US     26939022  R1XQ2N5CDOZGNX  B0094LZMT0       535858974   \n",
       "5          US      4772040  R1HCST57W334KN  B0112OSOQE        38517795   \n",
       "6          US     12910040  R32BUTYQS1ZJBQ  B000NPE5SA       373323715   \n",
       "7          US     38805573    RH4SXPL4L9QU  B00XWV4QXG       633842417   \n",
       "8          US     37100714  R37INWIQA5YW8N  B00X8UKOUK       666093513   \n",
       "9          US     41234409  R18GSVAAS3N8GX  B00OOKXTFU       801680808   \n",
       "\n",
       "                           product_title        product_category  star_rating  \\\n",
       "0                  Enlightened: Season 1  Digital_Video_Download            5   \n",
       "1                                Vicious  Digital_Video_Download            5   \n",
       "2                            After Words  Digital_Video_Download            4   \n",
       "3  Masterpiece: Inspector Lewis Season 5  Digital_Video_Download            5   \n",
       "4                      On The Waterfront  Digital_Video_Download            5   \n",
       "5                Rick and Morty Season 2  Digital_Video_Download            5   \n",
       "6                         Africa Screams  Digital_Video_Download            4   \n",
       "7                    Entourage: Season 7  Digital_Video_Download            3   \n",
       "8                 Catastrophe - Season 1  Digital_Video_Download            2   \n",
       "9         The Worricker Trilogy Season 1  Digital_Video_Download            3   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0              0            0    N                 Y   \n",
       "1              0            0    N                 Y   \n",
       "2             17           18    N                 Y   \n",
       "3              0            0    N                 Y   \n",
       "4              0            0    N                 Y   \n",
       "5              5            6    N                 Y   \n",
       "6              1            1    N                 Y   \n",
       "7              0            0    N                 Y   \n",
       "8              0            0    N                 Y   \n",
       "9              0            0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0         I loved it and I wish there was a season 3   \n",
       "1  As always it seems that the best shows come fr...   \n",
       "2                                     Charming movie   \n",
       "3                                         Five Stars   \n",
       "4               Brilliant film from beginning to end   \n",
       "5                          Best show on TV right now   \n",
       "6              Very funny. A typical mid 50's comedy   \n",
       "7                   it was not as good as the series   \n",
       "8                                          Two Stars   \n",
       "9                                       A tad Stuffy   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  I loved it and I wish there was a season 3... ...  2015-08-31  \n",
       "1  As always it seems that the best shows come fr...  2015-08-31  \n",
       "2  This movie isn't perfect, but it gets a lot of...  2015-08-31  \n",
       "3                excellant this is what tv should be  2015-08-31  \n",
       "4  Brilliant film from beginning to end. All of t...  2015-08-31  \n",
       "5  If you don't like this show. Go back to your n...  2015-08-31  \n",
       "6            Very funny.  A typical mid 50's comedy.  2015-08-31  \n",
       "7  Strange as it is, it was not as good as the se...  2015-08-31  \n",
       "8            Funny shows! We laughed out loud, alot!  2015-08-31  \n",
       "9  Well made. Great actors! Christopher Walken ma...  2015-08-31  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what is the information contained in all the columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of the dataset\n",
    "\n",
    "Get a little more comfortable with the data and see what features are at hand.\n",
    "\n",
    "- `marketplace`: Two-letter country code (in this case, all \"US\")\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author\n",
    "- `review_id`: Unique ID for the review\n",
    "- `product_id`: Amazon Standard Identification Number (ASIN). http://www.amazon.com/dp/<ASIN\\> links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN. Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case, digital videos)\n",
    "- `star_rating`: Product's rating (1 to 5 stars)\n",
    "- `helpful_votes`: Number of helpful votes for the review\n",
    "- `total_votes`: Number of total votes the review received\n",
    "- `vine`: Was the review written as part of the Vine program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: Title of the review itself\n",
    "- `review_body`: Text of the review\n",
    "- `review_date`: Date the review was written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing and understanding the dataset\n",
    "\n",
    "#### Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** You can refer [here](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html) to answer the following questions. \n",
    "\n",
    "**Question:** How many rows and columns do you have in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the dataset.  \n",
    "\n",
    "**Hint**: Use the `<dataframe>.shape` function to check the size of your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\t 15\n",
      "Data Points:\t 3998345\n"
     ]
    }
   ],
   "source": [
    "print(f'Features:\\t {df.shape[1]}')\n",
    "print(f'Data Points:\\t {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: 3998345 rows, 15 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which columns contain null values, and how many null values do they contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary of the dataset.  \n",
    "\n",
    "**Hint**: Use `<dataframe>.info` function using the keyword arguments `null_counts = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace            0\n",
       "customer_id            0\n",
       "review_id              0\n",
       "product_id             0\n",
       "product_parent         0\n",
       "product_title          0\n",
       "product_category       0\n",
       "star_rating            0\n",
       "helpful_votes          0\n",
       "total_votes            0\n",
       "vine                   0\n",
       "verified_purchase      0\n",
       "review_headline       25\n",
       "review_body           78\n",
       "review_date          138\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "#  - review_headline       25\n",
    "#  - review_body           78\n",
    "#  - review_date          138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Are there any duplicate rows? If yes, how many are there?  \n",
    "\n",
    "**Hint**: Filter the dataframe using `dataframe.duplicated()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html#pandas.DataFrame.duplicated)) and check the length of the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = df.duplicated()\n",
    "\n",
    "# Enter your code here\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: There are no duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Now it's time to decide what features you are going to use and how you are going to prepare them for your model. For this example, limit yourself to `customer_id`, `product_id`, `product_title`, and `star_rating`. Including additional features in the recommendation system could be beneficial but would require substantial processing (particularly the text data), which would be beyond the scope of this notebook.\n",
    "\n",
    "Reduce this dataset and only use the columns mentioned.  \n",
    "\n",
    "**Hint**: Select multiple columns as a dataframe by passing the columns as a list. For example: `df[['column_name 1', 'column_name 2']]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df[['customer_id', 'product_id', 'product_title', 'star_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check again if you have duplicates after reducing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = df_reduced.duplicated()\n",
    "\n",
    "# Enter your code here\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: Yes, there's 131 duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why do you have duplicates in your dataset now? What changed after you reduced the dataset? Review the first 20 lines of the duplicates. \n",
    "\n",
    "**Hint**: Use the `pandas.head(<number>)` function to print the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565194</th>\n",
       "      <td>41454255</td>\n",
       "      <td>B00Y2UYRFS</td>\n",
       "      <td>unseen 2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594322</th>\n",
       "      <td>17570065</td>\n",
       "      <td>B00R3EEO2G</td>\n",
       "      <td>The Maze Runner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611264</th>\n",
       "      <td>15703996</td>\n",
       "      <td>B00I3MQNWG</td>\n",
       "      <td>Bosch Season 1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612471</th>\n",
       "      <td>28456429</td>\n",
       "      <td>B008Y6W7J4</td>\n",
       "      <td>Rabbit Hole</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613791</th>\n",
       "      <td>52388381</td>\n",
       "      <td>B00YORA25I</td>\n",
       "      <td>McFarland, USA (Theatrical)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685156</th>\n",
       "      <td>31828958</td>\n",
       "      <td>B00TT53YSW</td>\n",
       "      <td>Bates Motel, Season 3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204110</th>\n",
       "      <td>19462</td>\n",
       "      <td>B00QWUL4AW</td>\n",
       "      <td>Exodus: Gods and Kings</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564816</th>\n",
       "      <td>24892653</td>\n",
       "      <td>B00L2GPYKW</td>\n",
       "      <td>The Escape Artist Season 1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601662</th>\n",
       "      <td>44513234</td>\n",
       "      <td>B00NY4UIKG</td>\n",
       "      <td>The Equalizer</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616429</th>\n",
       "      <td>43345475</td>\n",
       "      <td>B00P5968FC</td>\n",
       "      <td>The Babadook</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616584</th>\n",
       "      <td>20779855</td>\n",
       "      <td>B00O99GXPE</td>\n",
       "      <td>Max Lucado's The Christmas Candle</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617034</th>\n",
       "      <td>52861226</td>\n",
       "      <td>B00Q4FMCDS</td>\n",
       "      <td>The November Man</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618497</th>\n",
       "      <td>735926</td>\n",
       "      <td>B00IQBE0BU</td>\n",
       "      <td>Once Upon A Forest</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618598</th>\n",
       "      <td>3283400</td>\n",
       "      <td>B000NPQ0II</td>\n",
       "      <td>Creator</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618822</th>\n",
       "      <td>50380731</td>\n",
       "      <td>B00C58TGQ4</td>\n",
       "      <td>Hemingway &amp; Gellhorn</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618896</th>\n",
       "      <td>5465295</td>\n",
       "      <td>B009AP2B9Y</td>\n",
       "      <td>Kickin' It Volume 3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619053</th>\n",
       "      <td>39849456</td>\n",
       "      <td>B0035LN5YO</td>\n",
       "      <td>Gallipoli</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620635</th>\n",
       "      <td>22158610</td>\n",
       "      <td>B00M0HXNPK</td>\n",
       "      <td>Transcendence (2014)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621358</th>\n",
       "      <td>46543213</td>\n",
       "      <td>B005OPTSIQ</td>\n",
       "      <td>Workaholics Season 2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621751</th>\n",
       "      <td>25315183</td>\n",
       "      <td>B00NEFWXNK</td>\n",
       "      <td>Arrow: Season 3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id                      product_title  \\\n",
       "565194      41454255  B00Y2UYRFS                           unseen 2   \n",
       "594322      17570065  B00R3EEO2G                    The Maze Runner   \n",
       "611264      15703996  B00I3MQNWG                     Bosch Season 1   \n",
       "612471      28456429  B008Y6W7J4                        Rabbit Hole   \n",
       "613791      52388381  B00YORA25I        McFarland, USA (Theatrical)   \n",
       "685156      31828958  B00TT53YSW              Bates Motel, Season 3   \n",
       "1204110        19462  B00QWUL4AW             Exodus: Gods and Kings   \n",
       "1564816     24892653  B00L2GPYKW         The Escape Artist Season 1   \n",
       "1601662     44513234  B00NY4UIKG                      The Equalizer   \n",
       "1616429     43345475  B00P5968FC                       The Babadook   \n",
       "1616584     20779855  B00O99GXPE  Max Lucado's The Christmas Candle   \n",
       "1617034     52861226  B00Q4FMCDS                   The November Man   \n",
       "1618497       735926  B00IQBE0BU                 Once Upon A Forest   \n",
       "1618598      3283400  B000NPQ0II                            Creator   \n",
       "1618822     50380731  B00C58TGQ4               Hemingway & Gellhorn   \n",
       "1618896      5465295  B009AP2B9Y                Kickin' It Volume 3   \n",
       "1619053     39849456  B0035LN5YO                          Gallipoli   \n",
       "1620635     22158610  B00M0HXNPK               Transcendence (2014)   \n",
       "1621358     46543213  B005OPTSIQ               Workaholics Season 2   \n",
       "1621751     25315183  B00NEFWXNK                    Arrow: Season 3   \n",
       "\n",
       "         star_rating  \n",
       "565194             1  \n",
       "594322             2  \n",
       "611264             5  \n",
       "612471             5  \n",
       "613791             5  \n",
       "685156             5  \n",
       "1204110            5  \n",
       "1564816            5  \n",
       "1601662            5  \n",
       "1616429            3  \n",
       "1616584            5  \n",
       "1617034            2  \n",
       "1618497            3  \n",
       "1618598            4  \n",
       "1618822            3  \n",
       "1618896            5  \n",
       "1619053            5  \n",
       "1620635            5  \n",
       "1621358            5  \n",
       "1621751            5  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced[duplicates].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Take a look at the first two elements in the duplicates dataframe, and query the original dataframe df to see what the data looks like. You can use the `query` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html)).\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "df_eg = pd.DataFrame({\n",
    "            'A': [1,2,3,4],\n",
    "            'B': [\n",
    "        })\n",
    "df_eg.query('A > 1 & B > 0')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541362</th>\n",
       "      <td>US</td>\n",
       "      <td>41454255</td>\n",
       "      <td>R2F1FQI9DGBJVT</td>\n",
       "      <td>B00Y2UYRFS</td>\n",
       "      <td>542345963</td>\n",
       "      <td>unseen 2</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Do Not Waste Any Money On This Horrible Attemp...</td>\n",
       "      <td>Lorenzo Lamas has hit the bottom of the barrel...</td>\n",
       "      <td>2015-06-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565194</th>\n",
       "      <td>US</td>\n",
       "      <td>41454255</td>\n",
       "      <td>R2WZRCYMUC7LLV</td>\n",
       "      <td>B00Y2UYRFS</td>\n",
       "      <td>542345963</td>\n",
       "      <td>unseen 2</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Do NOT spend money on this movie!</td>\n",
       "      <td>Lorenzo Lamas has hit the bottom of the barrel...</td>\n",
       "      <td>2015-06-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "541362          US     41454255  R2F1FQI9DGBJVT  B00Y2UYRFS       542345963   \n",
       "565194          US     41454255  R2WZRCYMUC7LLV  B00Y2UYRFS       542345963   \n",
       "\n",
       "       product_title        product_category  star_rating  helpful_votes  \\\n",
       "541362      unseen 2  Digital_Video_Download            1              3   \n",
       "565194      unseen 2  Digital_Video_Download            1              1   \n",
       "\n",
       "        total_votes vine verified_purchase  \\\n",
       "541362            3    N                 Y   \n",
       "565194            1    N                 Y   \n",
       "\n",
       "                                          review_headline  \\\n",
       "541362  Do Not Waste Any Money On This Horrible Attemp...   \n",
       "565194                  Do NOT spend money on this movie!   \n",
       "\n",
       "                                              review_body review_date  \n",
       "541362  Lorenzo Lamas has hit the bottom of the barrel...  2015-06-16  \n",
       "565194  Lorenzo Lamas has hit the bottom of the barrel...  2015-06-12  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('customer_id == \"41454255\" & product_id == \"B00Y2UYRFS\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: That customer has made two reviews for the same movie, \n",
    "# with the exact same rating, but with different review titles,\n",
    "# that could mean they are different reviews, or the dataset has\n",
    "# all the history of changes of reviews with different review_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, remove the duplicate rows.\n",
    "\n",
    "**Hint**: Use the `~` operator to select all the rows that aren't duplicated. For example:\n",
    "    \n",
    "```\n",
    "df_eg = pd.DataFrame({\n",
    "            'A': [1,2,3,4],\n",
    "            'B': [2,0,5,2]\n",
    "        })\n",
    "df_eg[~(df_eg['B'] > 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998214, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = df_reduced[~duplicates]\n",
    "\n",
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some of the rows in the dataset\n",
    "If you haven't done so in the above, you can use the space below to further visualize some of your data. Look specifically at the distribution of features like `star_rating`, `customer_id`, and `product_id`.\n",
    "\n",
    "**Specific questions to consider**\n",
    "\n",
    "1. After looking at the distributions of features, to what extent might those features help your model? Is there anything you can deduce from those distributions that might be helpful in better understanding your data? \n",
    "\n",
    "2. Should you use all the data? What features should you use?\n",
    "\n",
    "3. What month has the highest count of user ratings?\n",
    "\n",
    "Use the cells below to visualize your data and answer these and other questions that might be of interest to you. Insert and delete cells where needed.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Include a summary of your answers to these and similar questions in your project presentations.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sns.barplot` ([documentation](https://seaborn.pydata.org/generated/seaborn.barplot.html)) to plot the `star_rating` density and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2410533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>756423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>345891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>289731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>195767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  star_rating\n",
       "0      5      2410533\n",
       "1      4       756423\n",
       "2      3       345891\n",
       "3      1       289731\n",
       "4      2       195767"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='index', ylabel='star_rating'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['star_rating'].value_counts().reset_index()\n",
    "\n",
    "sns.barplot(\n",
    "    x='index',\n",
    "    y='star_rating', # Enter your code here\n",
    "    data=_,  # The underscore symbol in Python is used to store the output of the last operation\n",
    "    palette='GnBu_d'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What month contains the highest count of user ratings?  \n",
    "\n",
    "**Hint**:  \n",
    "1. Use `pd.to_datetime` to convert the `review_date` column to a datetime column.  \n",
    "2. Use the month from the `review_date` column. You can access it for a datetime column using `<column_name>.dt.month`.  \n",
    "3. Use the `groupby` function using `idxmax`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_date</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>309083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>369059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>432084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>363611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>339285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>328114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>417844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>464912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>206453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>218583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>219316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>329863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_date  star_rating\n",
       "0           1.0       309083\n",
       "1           2.0       369059\n",
       "2           3.0       432084\n",
       "3           4.0       363611\n",
       "4           5.0       339285\n",
       "5           6.0       328114\n",
       "6           7.0       417844\n",
       "7           8.0       464912\n",
       "8           9.0       206453\n",
       "9          10.0       218583\n",
       "10         11.0       219316\n",
       "11         12.0       329863"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the review date to a datetime type. Here you will use original dataframe 'df'\n",
    "df['review_date'] = pd.to_datetime(df['review_date'])\n",
    "\n",
    "# Count the number of ratings by month\n",
    "df.groupby(df['review_date'].dt.month).star_rating.count().reset_index()\n",
    "\n",
    "# Use the bar plot again to plot the ratings(y) vs. review_date(x)\n",
    "g = sns.barplot(x='review_date', y='star_rating', data=_, palette='GnBu_d') # Enter your code here\n",
    "g.figure.set_size_inches((11, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The month with the most reviews is: August\n"
     ]
    }
   ],
   "source": [
    "# Use the Pandas groupby function on month to get the star rating count\n",
    "max_month = df.groupby(df['review_date'].dt.month_name()).star_rating.count().idxmax() # Enter your code here\n",
    "print(f'The month with the most reviews is: {max_month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: The month with most review is August"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus question (optional):** Which years have the most and least reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='review_date', ylabel='star_rating'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_date</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>1554812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_date  star_rating\n",
       "15       2015.0      1554812"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_date</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_date  star_rating\n",
       "0       2000.0            1\n",
       "1       2001.0            1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the Pandas groupby function on year and get the star rating count\n",
    "yearly_df = df.groupby(df['review_date'].dt.year).star_rating.count().reset_index() # Enter your code here\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "# Use the bar plot to plot star_rating(y) vs. review_date(x)\n",
    "sns.barplot(x='review_date', y='star_rating', data=yearly_df, palette='GnBu_d') # Enter your code here\n",
    "\n",
    "yearly_df[yearly_df.star_rating == yearly_df.star_rating.max()]\n",
    "yearly_df[yearly_df.star_rating == yearly_df.star_rating.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "#  - Most reviews: 2015 (1,554,812 reviews)\n",
    "#  - Least reviews: 2000 and 2001 (1 review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How heterogeneous are the number of reviews per customer and reviews per video? Use quantiles to find out.\n",
    "\n",
    "**Hint**: Use `<dataframe>['columns_name'].value_counts()` for the customers and products dataframe, and use `<dataframe>.quantile(<list>)` to find the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers\n",
      "        customer_id\n",
      "0.000          1.0\n",
      "0.010          1.0\n",
      "0.020          1.0\n",
      "0.030          1.0\n",
      "0.040          1.0\n",
      "0.050          1.0\n",
      "0.100          1.0\n",
      "0.250          1.0\n",
      "0.500          1.0\n",
      "0.750          2.0\n",
      "0.900          4.0\n",
      "0.950          5.0\n",
      "0.960          6.0\n",
      "0.970          7.0\n",
      "0.980          9.0\n",
      "0.990         13.0\n",
      "0.995         18.0\n",
      "0.999         37.0\n",
      "1.000       2704.0\n",
      "products\n",
      "        product_id\n",
      "0.000       1.000\n",
      "0.010       1.000\n",
      "0.020       1.000\n",
      "0.030       1.000\n",
      "0.040       1.000\n",
      "0.050       1.000\n",
      "0.100       1.000\n",
      "0.250       1.000\n",
      "0.500       3.000\n",
      "0.750       9.000\n",
      "0.900      31.000\n",
      "0.950      73.000\n",
      "0.960      95.000\n",
      "0.970     130.000\n",
      "0.980     199.000\n",
      "0.990     386.670\n",
      "0.995     699.000\n",
      "0.999    1993.901\n",
      "1.000   32790.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f57b41a3860>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f57b417c978>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 457.125x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 448.5x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "customers = pd.DataFrame(df['customer_id'].value_counts())\n",
    "products = pd.DataFrame(df['product_id'].value_counts())\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, \n",
    "             0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995, \n",
    "             0.999, 1]\n",
    "print('customers\\n', customers.quantile(quantiles)) # Enter your code here\n",
    "print('products\\n', products.quantile(quantiles)) # Enter your code here\n",
    "\n",
    "sns.relplot(data=customers.quantile(quantiles), kind='line')\n",
    "sns.relplot(data=products.quantile(quantiles), kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: The long tail is quite steep, with the majority of products and costumers having a low review count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out this long tail. Select the customers that have rated 18 or more videos and the products that have more than 95 reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11763902</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1411480</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35303629</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21285980</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29260449</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160037</th>\n",
       "      <td>35249628</td>\n",
       "      <td>B002KQ1K42</td>\n",
       "      <td>Best in Show</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160038</th>\n",
       "      <td>6579710</td>\n",
       "      <td>B0076PISBK</td>\n",
       "      <td>The Backyardigans</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160039</th>\n",
       "      <td>20783345</td>\n",
       "      <td>B009NPK1IO</td>\n",
       "      <td>Gossip Girl: The Complete Sixth Season</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160040</th>\n",
       "      <td>11123322</td>\n",
       "      <td>B009NPK1IO</td>\n",
       "      <td>Gossip Girl: The Complete Sixth Season</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160041</th>\n",
       "      <td>10030949</td>\n",
       "      <td>B002ET6P3Q</td>\n",
       "      <td>Imitation of Life (1959)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160042 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer_id  product_id                           product_title  \\\n",
       "0          11763902  B00PSLQYWE                  Downton Abbey Season 5   \n",
       "1           1411480  B00PSLQYWE                  Downton Abbey Season 5   \n",
       "2          35303629  B00PSLQYWE                  Downton Abbey Season 5   \n",
       "3          21285980  B00PSLQYWE                  Downton Abbey Season 5   \n",
       "4          29260449  B00PSLQYWE                  Downton Abbey Season 5   \n",
       "...             ...         ...                                     ...   \n",
       "160037     35249628  B002KQ1K42                            Best in Show   \n",
       "160038      6579710  B0076PISBK                       The Backyardigans   \n",
       "160039     20783345  B009NPK1IO  Gossip Girl: The Complete Sixth Season   \n",
       "160040     11123322  B009NPK1IO  Gossip Girl: The Complete Sixth Season   \n",
       "160041     10030949  B002ET6P3Q                Imitation of Life (1959)   \n",
       "\n",
       "        star_rating  \n",
       "0                 4  \n",
       "1                 5  \n",
       "2                 5  \n",
       "3                 5  \n",
       "4                 5  \n",
       "...             ...  \n",
       "160037            4  \n",
       "160038            5  \n",
       "160039            5  \n",
       "160040            5  \n",
       "160041            2  \n",
       "\n",
       "[160042 rows x 4 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers1 = customers[customers['customer_id'] > 18]\n",
    "products1 = products[products['product_id'] > 95]\n",
    "\n",
    "# Use the Pandas merge function to merge the customer1 and products1 with the original df_reduced dataset\n",
    "reduced_df = (\n",
    "            df_reduced.merge(pd.DataFrame({'customer_id': customers1.index}))\n",
    "                      .merge(pd.DataFrame({'product_id': products1.index}))\n",
    "            )# Enter your code here\n",
    "reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the shape of `customers1`, `products1`, and the new dataframe reduced_df?  \n",
    "\n",
    "**Note**: Use f-strings for this:\n",
    "```\n",
    "x= 3\n",
    "print(f'X = {x}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users is 9351 and number of items is 6612.\n",
      "Length of reduced df is (160042, 4).\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of users is {customers1.shape[0]} and number of items is {products1.shape[0]}.')# Enter your code here\n",
    "print(f'Length of reduced df is {reduced_df.shape}.')# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11763902</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1411480</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35303629</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21285980</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29260449</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id           product_title  star_rating\n",
       "0     11763902  B00PSLQYWE  Downton Abbey Season 5            4\n",
       "1      1411480  B00PSLQYWE  Downton Abbey Season 5            5\n",
       "2     35303629  B00PSLQYWE  Downton Abbey Season 5            5\n",
       "3     21285980  B00PSLQYWE  Downton Abbey Season 5            5\n",
       "4     29260449  B00PSLQYWE  Downton Abbey Season 5            5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does `reduced_df` maintain the same ratio of ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>79163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>39465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>22784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>10033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  star_rating\n",
       "0      5        79163\n",
       "1      4        39465\n",
       "2      3        22784\n",
       "3      2        10033\n",
       "4      1         8597"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='index', ylabel='star_rating'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduced_df['star_rating'].value_counts().reset_index()# Enter your code here\n",
    "sns.barplot(x='index', y='star_rating', data=_, palette='GnBu_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: Yes, it maintains roughly the same rate and pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recreate the customer and product distributions of count per customer and product.\n",
    "\n",
    "**Hint**: Use the `value_counts()` function on the `customer_id` and `product_id` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Distribution of counts per customer and product')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='customer_id'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='product_id'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle('Distribution of counts per customer and product')\n",
    "sns.distplot(customers, kde=False, ax=axs[0], color='teal')\n",
    "sns.distplot(products, kde=False, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, number each user and item, giving them their own sequential index. This will allow you to hold the information in a sparse format where the sequential indices indicate the row and column in the ratings matrix.\n",
    "\n",
    "To create the `customer_index` and `product_index`, create a new dataframe with `customer_id` as the index value and a sequential counter/values for the user and item number. Once you are finished creating both indexes, use the Pandas `merge` function to merge `customer_index` with `product_index`.\n",
    "\n",
    "**Hint**: Use the `shape` function to generate the total number of customers and products. Use `np.arange` to generate a list of numbers from 0 to the number of customers and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11763902</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>4</td>\n",
       "      <td>3664</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1411480</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35303629</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>4313</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21285980</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>412</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29260449</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id           product_title  star_rating  user  item\n",
       "0     11763902  B00PSLQYWE  Downton Abbey Season 5            4  3664   122\n",
       "1      1411480  B00PSLQYWE  Downton Abbey Season 5            5   136   122\n",
       "2     35303629  B00PSLQYWE  Downton Abbey Season 5            5  4313   122\n",
       "3     21285980  B00PSLQYWE  Downton Abbey Season 5            5   412   122\n",
       "4     29260449  B00PSLQYWE  Downton Abbey Season 5            5   131   122"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, \n",
    "                               'user': np.arange(customers.shape[0])}) # Enter your code here\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0])}) # Enter your code here\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)# Enter your code here\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample answer:\n",
    "<div class=\"output_subarea\"><div>\n",
    "\n",
    "<table class=\"dataframe\" border=\"1\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right\">\n",
    "      <th></th>\n",
    "      <th>customer_id</th>\n",
    "      <th>product_id</th>\n",
    "      <th>star_rating</th>\n",
    "      <th>product_title</th>\n",
    "      <th>user</th>\n",
    "      <th>item</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>11763902</td>\n",
    "      <td>B00PSLQYWE</td>\n",
    "      <td>4</td>\n",
    "      <td>Downton Abbey Season 5</td>\n",
    "      <td>3065</td>\n",
    "      <td>103</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>1411480</td>\n",
    "      <td>B00PSLQYWE</td>\n",
    "      <td>5</td>\n",
    "      <td>Downton Abbey Season 5</td>\n",
    "      <td>130</td>\n",
    "      <td>103</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>35303629</td>\n",
    "      <td>B00PSLQYWE</td>\n",
    "      <td>5</td>\n",
    "      <td>Downton Abbey Season 5</td>\n",
    "      <td>4683</td>\n",
    "      <td>103</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>21285980</td>\n",
    "      <td>B00PSLQYWE</td>\n",
    "      <td>5</td>\n",
    "      <td>Downton Abbey Season 5</td>\n",
    "      <td>449</td>\n",
    "      <td>103</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>29260449</td>\n",
    "      <td>B00PSLQYWE</td>\n",
    "      <td>5</td>\n",
    "      <td>Downton Abbey Season 5</td>\n",
    "      <td>131</td>\n",
    "      <td>103</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> End of Lab 2 </span>\n",
    "\n",
    "Save the project file to your local computer. Follow these steps:\n",
    "\n",
    "1. At the top of the page, click the **File** menu. \n",
    "\n",
    "1. Select **Download as**, and click **Notebook(.ipynb)**.  \n",
    "\n",
    "This downloads the current notebook to the default download folder on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model training and evaluation\n",
    "\n",
    "There are some preliminary steps that you must include when converting the dataset from a dataframe to a format that a machine learning algorithm can use. For Amazon SageMaker, here are the steps you need to take:\n",
    "\n",
    "1. Split the data into `train_data` and `test_data`.    \n",
    "2. Convert the dataset to an appropriate file format that the Amazon SageMaker training job can use. This can be either a CSV file or record protobuf. For more information, see [Common Data Formats for Training](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html). For this problem, the data will be sparse, so you can use the `scipy.sparse.lilmatrix` function and then convert the function to the `RecordIO protobuf` format using `sagemaker.amazon.common.write_spmatrix_to_sparse_tensor`.    \n",
    "3. Upload the data to your Amazon S3 bucket. If you have not created one before, see [Create a Bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html).    \n",
    "\n",
    "Use the following cells to complete these steps. Insert and delete cells where needed.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Take note of the key decisions you've made in this phase in your project presentations.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "You are at a point where you can start preparing the dataset as input for your model. Every model has different input needs. Some of the algorithms implemented in Amazon SageMaker require the data to be in the recordIO-wrapped protobuf form. You will take care of that in the following cells.\n",
    "\n",
    "First, split the dataset into training and test sets. This will allow you to estimate the model's accuracy on videos that customers rated but that weren't included in the training.\n",
    "\n",
    "Start with creation of the `test_df` dataframe. Create the dataframe by grouping the dataframe on `customer_id` and using the `last` function, similar to `pd.groupby('  ').last()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby(['customer_id']).last().reset_index() # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the training data, remove the values present in `test_df` from the `reduced_df` dataframe.\n",
    "\n",
    "**Hint**: Merge the `reduced_df` dataframe with the `test_df` dataset with `customer_id` and `product_id` columns as an outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "train_df = reduced_df.merge(test_df, \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')].reset_index()\n",
    "train_df = train_df[['customer_id', 'product_id', 'product_title_x', 'star_rating_x', 'user_x', 'item_x']]\n",
    "train_df.rename(columns={ \n",
    "    'product_title_x': 'product_title', \n",
    "    'star_rating_x': 'star_rating', \n",
    "    'user_x': 'user', \n",
    "    'item_x': 'item' }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10981</td>\n",
       "      <td>B0013UIUBC</td>\n",
       "      <td>No Reservations</td>\n",
       "      <td>4</td>\n",
       "      <td>1754</td>\n",
       "      <td>4250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12783</td>\n",
       "      <td>B001GQK56G</td>\n",
       "      <td>Inspector Morse Season 1</td>\n",
       "      <td>4</td>\n",
       "      <td>2103</td>\n",
       "      <td>2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19462</td>\n",
       "      <td>B00DFFI7K2</td>\n",
       "      <td>G.I. Joe: Retaliation</td>\n",
       "      <td>1</td>\n",
       "      <td>8056</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54160</td>\n",
       "      <td>B005OPTSIQ</td>\n",
       "      <td>Workaholics Season 2</td>\n",
       "      <td>2</td>\n",
       "      <td>6663</td>\n",
       "      <td>3756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61561</td>\n",
       "      <td>B00125FBTM</td>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>5</td>\n",
       "      <td>8821</td>\n",
       "      <td>3653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id             product_title  star_rating  user  item\n",
       "0        10981  B0013UIUBC           No Reservations            4  1754  4250\n",
       "1        12783  B001GQK56G  Inspector Morse Season 1            4  2103  2833\n",
       "2        19462  B00DFFI7K2     G.I. Joe: Retaliation            1  8056   623\n",
       "3        54160  B005OPTSIQ      Workaholics Season 2            2  6663  3756\n",
       "4        61561  B00125FBTM        A Clockwork Orange            5  8821  3653"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11763902</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>4</td>\n",
       "      <td>3664</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1411480</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35303629</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>4313</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21285980</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>412</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29260449</td>\n",
       "      <td>B00PSLQYWE</td>\n",
       "      <td>Downton Abbey Season 5</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id           product_title  star_rating  user  item\n",
       "0     11763902  B00PSLQYWE  Downton Abbey Season 5            4  3664   122\n",
       "1      1411480  B00PSLQYWE  Downton Abbey Season 5            5   136   122\n",
       "2     35303629  B00PSLQYWE  Downton Abbey Season 5            5  4313   122\n",
       "3     21285980  B00PSLQYWE  Downton Abbey Season 5            5   412   122\n",
       "4     29260449  B00PSLQYWE  Downton Abbey Season 5            5   131   122"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can look at some basic characteristics of the data that will later help you convert the features to an appropriate format for training your model.\n",
    "\n",
    "Create two variables `nb_rating_test` and `nb_ratings_train` for the length of the test and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Count: 150784\n",
      " Test Count: 9258\n"
     ]
    }
   ],
   "source": [
    "nb_ratings_test =  test_df.shape[0]\n",
    "nb_ratings_train = train_df.shape[0]\n",
    "print(f\" Training Count: {nb_ratings_train}\")\n",
    "print(f\" Test Count: {nb_ratings_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Now, you can convert your Pandas dataframes into a sparse matrix. This process is the same for both train and test. The Amazon SageMaker implementation of factorization machines takes recordIO-wrapped protobuf, where the data you have today is a Pandas dataframe on disk. Therefore, you are going to convert the data to a sparse matrix to express the relationships between each user and each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def loadDataset(df, lines, columns, regressor=True):\n",
    "    \"\"\"\n",
    "    Convert the pandas dataframe into a sparse matrix\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        lines: number of rows of the final sparse matrix\n",
    "        columns: number of columns of final sparse matrix\n",
    "        regressor: Boolean value to check if using regression\n",
    "                  or classification\n",
    "    Returns:\n",
    "        X: Feature vector\n",
    "        Y: Label vector\n",
    "    \"\"\"\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    \n",
    "    # Use scipy.sparse.lil_matrix to create the feature vector X of type float32\n",
    "    # The size of the matrix is the length of the dataframe and \n",
    "    # number of lines plus number of columns variable \n",
    "    X = lil_matrix((df.shape[0], lines + columns)).astype('float32') # Enter your code here\n",
    "    \n",
    "    # Labels are stored in a vector. Instantiate an empty label vector Y.\n",
    "    Y = []\n",
    "    \n",
    "    line = 0\n",
    "    \n",
    "    # For each row in the dataframe, use 1 for the item and product number\n",
    "    for index, row in df.iterrows():\n",
    "        X[line, row['user']] = 1\n",
    "        X[line, lines + (row['item'])] = 1\n",
    "        line += 1\n",
    "\n",
    "        if regressor:\n",
    "            # If using regression, append the star_rating from the row variable\n",
    "            Y.append(row['star_rating']) # Enter your code here\n",
    "        else:\n",
    "            # Use 1 for star_rating 5 else use 0 from the row variable\n",
    "            if int(row['star_rating']) >= 5:\n",
    "                Y.append(1) # Enter your code here\n",
    "            else:\n",
    "                Y.append(0) # Enter your code here\n",
    "            \n",
    "    # Convert the list into a NumPy array of type float32     \n",
    "    Y = np.array(Y).astype('float32') # Enter your code here\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `loadDataset` function to create the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9258 6583 15841\n"
     ]
    }
   ],
   "source": [
    "print(customers.shape[0], \n",
    "      products.shape[0],\n",
    "      customers.shape[0] + products.shape[0])\n",
    "\n",
    "# Use loadDataset function with train_df, customers.shape[0] and products.shape[0]\n",
    "X_train, Y_train = loadDataset(train_df, customers.shape[0], products.shape[0])  # Enter your code here\n",
    "\n",
    "# Use loadDataset function with test_df, customers.shape[0] and products.shape[0]\n",
    "X_test, Y_test = loadDataset(test_df, customers.shape[0], products.shape[0]) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your data is in a sparse format, save it as a protobuf format and upload it to Amazon S3. This step might look intimidating, but most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as SageMaker below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data S3 path:  s3://qls-4019912-5ead0778b062168c-labbucket-1h2g0kyyebu6d/sagemaker-fm/train\n",
      "Test data S3 path:  s3://qls-4019912-5ead0778b062168c-labbucket-1h2g0kyyebu6d/sagemaker-fm/test\n"
     ]
    }
   ],
   "source": [
    "import io \n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToProtobuf(X, bucket, prefix, key, d_type, Y=None):\n",
    "    buf = io.BytesIO()\n",
    "    if d_type == \"sparse\":\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, X, labels=Y)\n",
    "    else:\n",
    "        smac.write_numpy_to_dense_tensor(buf, X, labels=Y)\n",
    "        \n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "\n",
    "\n",
    "fm_train_data_path = writeDatasetToProtobuf(X_train, bucket, prefix, 'train', \"sparse\", Y_train)    \n",
    "fm_test_data_path  = writeDatasetToProtobuf(X_test, bucket, prefix, 'test', \"sparse\", Y_test)  \n",
    "  \n",
    "print(\"Training data S3 path: \", fm_train_data_path)\n",
    "print(\"Test data S3 path: \", fm_test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are finally finished with data preparation. Hooray! As you can see, it takes a lot of time and effort to clean and prepare the data for modeling. This is true for every single data science project, and this step has a high impact on the outcome. Make sure you spend enough time understanding and preparing your data for training in all future machine learning dventures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now it's time to train the model. You will use an Amazon SageMaker training job for that. Amazon SageMaker training jobs are an easy way to create models, as you don't really have to write all the code for training. That is already handled for you in a nice container format.\n",
    "\n",
    "The general workflow for creating training jobs from the notebook is to instantiate the predictor, pass some hyperparameters, and then pass the data in the correct format. This is what happens in the following cell.\n",
    "\n",
    "For more more information about FM estimator, see [FactorizationMachines](https://sagemaker.readthedocs.io/en/stable/factorization_machines.html).\n",
    "\n",
    "For more information about hyperparameters, see [Factorization Machines Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html).\n",
    "\n",
    "**Hint**: Example:\n",
    "\n",
    "```\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "pca = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                    role,\n",
    "                                    instance_count=1,\n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=sess)\n",
    "                                    \n",
    "pca.set_hyperparameters(featuer_dim=50000,\n",
    "                        num_components=10,\n",
    "                        subtract_mean=True,\n",
    "                        algorithm_mode='randomized',\n",
    "                        mini_batch_size=200)\n",
    "                        \n",
    "pca.fit({'train': s3_train_data})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 19:51:56 Starting - Starting the training job...\n",
      "2021-03-11 19:52:19 Starting - Launching requested ML instancesProfilerReport-1615492316: InProgress\n",
      "......\n",
      "2021-03-11 19:53:20 Starting - Preparing the instances for training......\n",
      "2021-03-11 19:54:25 Downloading - Downloading input data...\n",
      "2021-03-11 19:54:47 Training - Downloading the training image...\n",
      "2021-03-11 19:55:22 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '15841', 'predictor_type': 'regressor', 'rescale_grad': '0.005', 'clip_gradient': '5.0', 'num_factors': '64', 'epochs': '25', 'mini_batch_size': '200'}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] Final configuration: {'epochs': '25', 'mini_batch_size': '200', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '15841', 'predictor_type': 'regressor', 'rescale_grad': '0.005', 'clip_gradient': '5.0', 'num_factors': '64'}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 WARNING 140325587183424] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:11.256] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:11.260] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 6, \"num_examples\": 1, \"num_bytes\": 12800}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] nvidia-smi took: 0.025313138961791992 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492511.252953, \"EndTime\": 1615492511.2954333, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 36.800384521484375, \"count\": 1, \"min\": 36.800384521484375, \"max\": 36.800384521484375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492511.2956324, \"EndTime\": 1615492511.295687, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[19:55:11] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204326.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[19:55:11] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204326.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=4.442522053661143\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=19.736002197265623\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:11 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=4.302616577148438\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:12 INFO 140325587183424] Iter[0] Batch [500]#011Speed: 63230.29 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:12 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, batch=500 train rmse <loss>=1.3582076022151959\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:12 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, batch=500 train mse <loss>=1.8447278907151516\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:12 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, batch=500 train absolute_loss <loss>=1.0448019343697859\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:14.028] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2675, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.3068298159172311\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, train mse <loss>=1.7078041677702642\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.0172580866725123\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492511.2955256, \"EndTime\": 1615492514.0295396, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"update.time\": {\"sum\": 2733.553171157837, \"count\": 1, \"min\": 2733.553171157837, \"max\": 2733.553171157837}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492511.2959502, \"EndTime\": 1615492514.0298994, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 150984.0, \"count\": 1, \"min\": 150984, \"max\": 150984}, \"Total Batches Seen\": {\"sum\": 755.0, \"count\": 1, \"min\": 755, \"max\": 755}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=55149.13310772584 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.1319910814992096\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.28140380859375\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:14 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.9580535888671875\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:15 INFO 140325587183424] Iter[1] Batch [500]#011Speed: 65044.71 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:15 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, batch=500 train rmse <loss>=1.1295291317328906\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:15 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, batch=500 train mse <loss>=1.2758360594332576\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:15 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, batch=500 train absolute_loss <loss>=0.9033141724363772\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:16.257] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2223, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.1468962104900584\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.3153709176364565\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.9188905600590794\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492514.02966, \"EndTime\": 1615492516.2578864, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2226.839542388916, \"count\": 1, \"min\": 2226.839542388916, \"max\": 2226.839542388916}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492514.0310068, \"EndTime\": 1615492516.258173, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 301768.0, \"count\": 1, \"min\": 301768, \"max\": 301768}, \"Total Batches Seen\": {\"sum\": 1509.0, \"count\": 1, \"min\": 1509, \"max\": 1509}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=67697.24239910692 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.1165645457776703\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.2467163848876952\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:16 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.9397788238525391\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:17 INFO 140325587183424] Iter[2] Batch [500]#011Speed: 74198.15 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:17 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, batch=500 train rmse <loss>=1.1133067138230466\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:17 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, batch=500 train mse <loss>=1.2394518390434708\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:17 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, batch=500 train absolute_loss <loss>=0.8899363198346958\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:18.303] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2042, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.1307180929469562\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.2785234057176018\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.9059330341733736\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492516.2579854, \"EndTime\": 1615492518.3042395, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2045.1970100402832, \"count\": 1, \"min\": 2045.1970100402832, \"max\": 2045.1970100402832}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492516.2590055, \"EndTime\": 1615492518.3045406, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 452552.0, \"count\": 1, \"min\": 452552, \"max\": 452552}, \"Total Batches Seen\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=73708.09249473005 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.1074063050051839\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.2263487243652345\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:18 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.9294747924804687\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:19 INFO 140325587183424] Iter[3] Batch [500]#011Speed: 68925.51 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:19 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, batch=500 train rmse <loss>=1.0988770241022217\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:19 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, batch=500 train mse <loss>=1.2075307140997547\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:19 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, batch=500 train absolute_loss <loss>=0.8775851508957183\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:20.456] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2148, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.1165046948081694\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.2465827335286837\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.8941707862024282\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492518.3043365, \"EndTime\": 1615492520.4567637, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2151.276111602783, \"count\": 1, \"min\": 2151.276111602783, \"max\": 2151.276111602783}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492518.305457, \"EndTime\": 1615492520.4569974, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 603336.0, \"count\": 1, \"min\": 603336, \"max\": 603336}, \"Total Batches Seen\": {\"sum\": 3017.0, \"count\": 1, \"min\": 3017, \"max\": 3017}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=70077.94859652725 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.100015778861884\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.2100347137451173\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:20 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.9213760375976563\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:21 INFO 140325587183424] Iter[4] Batch [500]#011Speed: 72801.64 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:21 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, batch=500 train rmse <loss>=1.0862224669116025\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:21 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, batch=500 train mse <loss>=1.1798792476235274\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:21 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, batch=500 train absolute_loss <loss>=0.8666853285311701\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:22.516] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2056, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.104080201241634\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.2189930907737672\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.8837767571163431\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492520.456844, \"EndTime\": 1615492522.5175729, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2059.628486633301, \"count\": 1, \"min\": 2059.628486633301, \"max\": 2059.628486633301}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492520.4579096, \"EndTime\": 1615492522.5178587, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 754120.0, \"count\": 1, \"min\": 754120, \"max\": 754120}, \"Total Batches Seen\": {\"sum\": 3771.0, \"count\": 1, \"min\": 3771, \"max\": 3771}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=73192.89103420149 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.0934990644062856\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.1957402038574219\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:22 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.914842529296875\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:23 INFO 140325587183424] Iter[5] Batch [500]#011Speed: 73891.98 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:23 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, batch=500 train rmse <loss>=1.07504390348364\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:23 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, batch=500 train mse <loss>=1.155719394417342\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:23 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, batch=500 train absolute_loss <loss>=0.8570232189582018\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:24.646] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2125, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.0931084084906384\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.1948859927129365\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.8745166676872921\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492522.5176675, \"EndTime\": 1615492524.6467469, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2128.0550956726074, \"count\": 1, \"min\": 2128.0550956726074, \"max\": 2128.0550956726074}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492522.5186617, \"EndTime\": 1615492524.6469898, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 904904.0, \"count\": 1, \"min\": 904904, \"max\": 904904}, \"Total Batches Seen\": {\"sum\": 4525.0, \"count\": 1, \"min\": 4525, \"max\": 4525}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=70842.14903384738 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=1.0874915462472248\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=1.1826378631591796\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:24 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.9094041442871094\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] Iter[6] Batch [500]#011Speed: 74217.13 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, batch=500 train rmse <loss>=1.0650664038315056\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, batch=500 train mse <loss>=1.1343664445705757\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, batch=500 train absolute_loss <loss>=0.8483520593091162\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:26.673] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2024, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.0833044943570505\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.1735486274941846\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.8661545784340613\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492524.646836, \"EndTime\": 1615492526.6745684, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2026.5648365020752, \"count\": 1, \"min\": 2026.5648365020752, \"max\": 2026.5648365020752}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492524.6479666, \"EndTime\": 1615492526.674835, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1055688.0, \"count\": 1, \"min\": 1055688, \"max\": 1055688}, \"Total Batches Seen\": {\"sum\": 5279.0, \"count\": 1, \"min\": 5279, \"max\": 5279}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=74387.68824385903 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=1.0818116950648582\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=1.1703165435791016\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:26 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.9044331359863281\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] Iter[7] Batch [500]#011Speed: 74490.49 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, batch=500 train rmse <loss>=1.0560669788033346\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, batch=500 train mse <loss>=1.1152774637188025\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, batch=500 train absolute_loss <loss>=0.840465582126153\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:28.686] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2009, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.0744443959302772\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.1544307599459782\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.8585073815606317\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492526.6746504, \"EndTime\": 1615492528.687123, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2011.4033222198486, \"count\": 1, \"min\": 2011.4033222198486, \"max\": 2011.4033222198486}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492526.6756856, \"EndTime\": 1615492528.687435, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1206472.0, \"count\": 1, \"min\": 1206472, \"max\": 1206472}, \"Total Batches Seen\": {\"sum\": 6033.0, \"count\": 1, \"min\": 6033, \"max\": 6033}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=74945.1580353809 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=1.0763638738097696\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=1.1585591888427735\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:28 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.89967529296875\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] Iter[8] Batch [500]#011Speed: 68741.63 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, batch=500 train rmse <loss>=1.0478695759778789\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, batch=500 train mse <loss>=1.0980306482600595\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, batch=500 train absolute_loss <loss>=0.8332140307702466\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:30.823] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2132, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.0663545840343913\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.1371120988911596\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.8514394813760205\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492528.687219, \"EndTime\": 1615492530.8239117, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2135.557174682617, \"count\": 1, \"min\": 2135.557174682617, \"max\": 2135.557174682617}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492528.6883235, \"EndTime\": 1615492530.8241584, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1357256.0, \"count\": 1, \"min\": 1357256, \"max\": 1357256}, \"Total Batches Seen\": {\"sum\": 6787.0, \"count\": 1, \"min\": 6787, \"max\": 6787}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=70593.20496894133 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=1.0710926815503423\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=1.1472395324707032\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:30 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.8950891876220703\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] Iter[9] Batch [500]#011Speed: 75316.29 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, batch=500 train rmse <loss>=1.040336805270506\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, batch=500 train mse <loss>=1.0823006684004428\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, batch=500 train absolute_loss <loss>=0.8264956897271132\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:32.839] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 2013, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.0589013122686208\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.1212719891242071\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.8448554361783541\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492530.8240004, \"EndTime\": 1615492532.8405478, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2015.4821872711182, \"count\": 1, \"min\": 2015.4821872711182, \"max\": 2015.4821872711182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492530.8250394, \"EndTime\": 1615492532.8408468, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1508040.0, \"count\": 1, \"min\": 1508040, \"max\": 1508040}, \"Total Batches Seen\": {\"sum\": 7541.0, \"count\": 1, \"min\": 7541, \"max\": 7541}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=74795.33908783439 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=1.0659646664716498\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=1.1362806701660155\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:32 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=0.89060791015625\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] Iter[10] Batch [500]#011Speed: 76135.07 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, batch=500 train rmse <loss>=1.0333613803295219\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, batch=500 train mse <loss>=1.0678357423565346\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, batch=500 train absolute_loss <loss>=0.8202174713987552\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:34.913] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2069, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=1.051981003312215\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, train mse <loss>=1.1066640313297746\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=0.8386752799967556\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492532.840642, \"EndTime\": 1615492534.9145794, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2072.828531265259, \"count\": 1, \"min\": 2072.828531265259, \"max\": 2072.828531265259}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492532.8417206, \"EndTime\": 1615492534.9148269, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1658824.0, \"count\": 1, \"min\": 1658824, \"max\": 1658824}, \"Total Batches Seen\": {\"sum\": 8295.0, \"count\": 1, \"min\": 8295, \"max\": 8295}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=72729.08424644223 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=1.0609576351182102\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=1.125631103515625\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:34 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=0.8862525177001953\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:36 INFO 140325587183424] Iter[11] Batch [500]#011Speed: 75112.13 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:36 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, batch=500 train rmse <loss>=1.0268590111549516\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:36 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, batch=500 train mse <loss>=1.0544394287901249\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:36 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, batch=500 train absolute_loss <loss>=0.8143149668680217\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:37.075] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2157, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=1.0455127002828921\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, train mse <loss>=1.0930968064528246\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=0.8328393959429915\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492534.914668, \"EndTime\": 1615492537.0757203, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2160.0377559661865, \"count\": 1, \"min\": 2160.0377559661865, \"max\": 2160.0377559661865}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492534.9156525, \"EndTime\": 1615492537.07601, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1809608.0, \"count\": 1, \"min\": 1809608, \"max\": 1809608}, \"Total Batches Seen\": {\"sum\": 9049.0, \"count\": 1, \"min\": 9049, \"max\": 9049}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=69792.00519814392 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=1.0560561504463235\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=1.1152545928955078\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:37 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=0.8821327209472656\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:38 INFO 140325587183424] Iter[12] Batch [500]#011Speed: 73982.38 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:38 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, batch=500 train rmse <loss>=1.0207627197989089\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:38 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, batch=500 train mse <loss>=1.0419565301312659\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:38 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, batch=500 train absolute_loss <loss>=0.8087301479651781\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:39.104] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2025, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=1.0394323245665695\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, train mse <loss>=1.0804195573538622\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=0.8272983663847339\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492537.0757985, \"EndTime\": 1615492539.105221, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2028.2247066497803, \"count\": 1, \"min\": 2028.2247066497803, \"max\": 2028.2247066497803}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492537.076962, \"EndTime\": 1615492539.1055734, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1960392.0, \"count\": 1, \"min\": 1960392, \"max\": 1960392}, \"Total Batches Seen\": {\"sum\": 9803.0, \"count\": 1, \"min\": 9803, \"max\": 9803}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=74323.35580573641 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=1.0512487342999217\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=1.1051239013671874\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:39 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=0.8780189514160156\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:40 INFO 140325587183424] Iter[13] Batch [500]#011Speed: 72659.15 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:40 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, batch=500 train rmse <loss>=1.0150186398988286\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:40 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, batch=500 train mse <loss>=1.0302628393420679\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:40 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, batch=500 train absolute_loss <loss>=0.8034246262723577\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:41.159] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2050, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=1.0336884427344042\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, train mse <loss>=1.0685117966426778\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=0.8220162115931827\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492539.1053157, \"EndTime\": 1615492541.1597815, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2053.3132553100586, \"count\": 1, \"min\": 2053.3132553100586, \"max\": 2053.3132553100586}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492539.106438, \"EndTime\": 1615492541.1600115, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2111176.0, \"count\": 1, \"min\": 2111176, \"max\": 2111176}, \"Total Batches Seen\": {\"sum\": 10557.0, \"count\": 1, \"min\": 10557, \"max\": 10557}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=73420.52861290825 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=1.0465269871060234\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=1.095218734741211\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:41 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=0.8739076232910157\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:42 INFO 140325587183424] Iter[14] Batch [500]#011Speed: 75839.03 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:42 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, batch=500 train rmse <loss>=1.0095828425286488\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:42 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, batch=500 train mse <loss>=1.0192575159282267\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:42 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, batch=500 train absolute_loss <loss>=0.7983624384836284\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:43.172] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2009, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=1.0282392044623632\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, train mse <loss>=1.0572758615933933\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=0.8169621583447848\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492541.1598582, \"EndTime\": 1615492543.1731753, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2012.169361114502, \"count\": 1, \"min\": 2012.169361114502, \"max\": 2012.169361114502}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492541.160974, \"EndTime\": 1615492543.17352, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2261960.0, \"count\": 1, \"min\": 2261960, \"max\": 2261960}, \"Total Batches Seen\": {\"sum\": 11311.0, \"count\": 1, \"min\": 11311, \"max\": 11311}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=74918.0712659598 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=1.0418843115726295\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=1.085522918701172\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:43 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=0.8698435211181641\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:44 INFO 140325587183424] Iter[15] Batch [500]#011Speed: 75861.07 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:44 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, batch=500 train rmse <loss>=1.004419019197356\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:44 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, batch=500 train mse <loss>=1.0088575661253787\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:44 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, batch=500 train absolute_loss <loss>=0.79351782176309\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:45.258] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2082, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=1.023050066114659\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, train mse <loss>=1.0466314377772081\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=0.8121131857004342\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492543.1733065, \"EndTime\": 1615492545.259331, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2084.8960876464844, \"count\": 1, \"min\": 2084.8960876464844, \"max\": 2084.8960876464844}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492543.1743486, \"EndTime\": 1615492545.2596705, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2412744.0, \"count\": 1, \"min\": 2412744, \"max\": 2412744}, \"Total Batches Seen\": {\"sum\": 12065.0, \"count\": 1, \"min\": 12065, \"max\": 12065}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=72301.82107997165 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=1.0373150028612972\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=1.0760224151611328\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:45 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=0.8657706451416015\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:46 INFO 140325587183424] Iter[16] Batch [500]#011Speed: 76451.31 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:46 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, batch=500 train rmse <loss>=0.9994968635050574\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:46 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, batch=500 train mse <loss>=0.9989939801564474\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:46 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, batch=500 train absolute_loss <loss>=0.7888725423908043\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:47.248] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 1982, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=1.01809221111601\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, train mse <loss>=1.0365117503350862\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=0.8074510434704687\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492545.2594395, \"EndTime\": 1615492547.2490242, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1988.203525543213, \"count\": 1, \"min\": 1988.203525543213, \"max\": 1988.203525543213}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492545.2607853, \"EndTime\": 1615492547.2492647, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2563528.0, \"count\": 1, \"min\": 2563528, \"max\": 2563528}, \"Total Batches Seen\": {\"sum\": 12819.0, \"count\": 1, \"min\": 12819, \"max\": 12819}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=75821.75199465008 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=1.0328145683602588\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=1.0667059326171875\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:47 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=0.8616934204101563\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:48 INFO 140325587183424] Iter[17] Batch [500]#011Speed: 75981.13 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:48 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, batch=500 train rmse <loss>=0.994790749573013\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:48 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, batch=500 train mse <loss>=0.989608635436037\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:48 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, batch=500 train absolute_loss <loss>=0.784404693116209\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:49.259] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2007, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=1.0133412976903693\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, train mse <loss>=1.0268605856048016\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=0.8029552371179394\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492547.2491019, \"EndTime\": 1615492549.2602997, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2009.9351406097412, \"count\": 1, \"min\": 2009.9351406097412, \"max\": 2009.9351406097412}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492547.2503307, \"EndTime\": 1615492549.2606018, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2714312.0, \"count\": 1, \"min\": 2714312, \"max\": 2714312}, \"Total Batches Seen\": {\"sum\": 13573.0, \"count\": 1, \"min\": 13573, \"max\": 13573}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=75001.04470896852 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=1.0283792755234675\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=1.0575639343261718\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:49 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=0.8576156616210937\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:50 INFO 140325587183424] Iter[18] Batch [500]#011Speed: 70734.17 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:50 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, batch=500 train rmse <loss>=0.9902788156375665\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:50 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, batch=500 train mse <loss>=0.9806521327005413\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:50 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, batch=500 train absolute_loss <loss>=0.7800974968855015\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:51.360] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2097, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=1.00877656707371\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, train mse <loss>=1.0176301622770194\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=0.798611660560183\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492549.2603917, \"EndTime\": 1615492551.3617003, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2100.212812423706, \"count\": 1, \"min\": 2100.212812423706, \"max\": 2100.212812423706}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492549.261453, \"EndTime\": 1615492551.3620093, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2865096.0, \"count\": 1, \"min\": 2865096, \"max\": 2865096}, \"Total Batches Seen\": {\"sum\": 14327.0, \"count\": 1, \"min\": 14327, \"max\": 14327}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=71777.94139970143 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=1.0240056867441583\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=1.048587646484375\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:51 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=0.8535405731201172\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:52 INFO 140325587183424] Iter[19] Batch [500]#011Speed: 76216.89 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:52 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, batch=500 train rmse <loss>=0.9859422434089955\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:52 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, batch=500 train mse <loss>=0.9720821073383629\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:52 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, batch=500 train absolute_loss <loss>=0.7759414050012767\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:53.366] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 2001, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=1.0043801578069937\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, train mse <loss>=1.0087795013964018\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=0.7944117164105888\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492551.3618033, \"EndTime\": 1615492553.3670745, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2004.2071342468262, \"count\": 1, \"min\": 2004.2071342468262, \"max\": 2004.2071342468262}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492551.362835, \"EndTime\": 1615492553.3673723, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3015880.0, \"count\": 1, \"min\": 3015880, \"max\": 3015880}, \"Total Batches Seen\": {\"sum\": 15081.0, \"count\": 1, \"min\": 15081, \"max\": 15081}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=75215.62192881563 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, batch=0 train rmse <loss>=1.0196907053226896\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, batch=0 train mse <loss>=1.0397691345214843\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:53 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, batch=0 train absolute_loss <loss>=0.8494705200195313\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:54 INFO 140325587183424] Iter[20] Batch [500]#011Speed: 76003.07 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:54 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, batch=500 train rmse <loss>=0.9817646880478007\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:54 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, batch=500 train mse <loss>=0.9638619026975955\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:54 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, batch=500 train absolute_loss <loss>=0.771927191530635\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:55.433] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 2063, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, train rmse <loss>=1.000136555282708\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, train mse <loss>=1.0002731292127613\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #quality_metric: host=algo-1, epoch=20, train absolute_loss <loss>=0.7903459034927328\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492553.3671715, \"EndTime\": 1615492555.4338324, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2065.5388832092285, \"count\": 1, \"min\": 2065.5388832092285, \"max\": 2065.5388832092285}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492553.3682623, \"EndTime\": 1615492555.434065, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3166664.0, \"count\": 1, \"min\": 3166664, \"max\": 3166664}, \"Total Batches Seen\": {\"sum\": 15835.0, \"count\": 1, \"min\": 15835, \"max\": 15835}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=72986.29783897598 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, batch=0 train rmse <loss>=1.0154317724841548\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, batch=0 train mse <loss>=1.0311016845703125\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:55 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, batch=0 train absolute_loss <loss>=0.8454082489013672\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:56 INFO 140325587183424] Iter[21] Batch [500]#011Speed: 74556.60 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:56 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, batch=500 train rmse <loss>=0.9777318773507404\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:56 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, batch=500 train mse <loss>=0.9559596239878032\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:56 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, batch=500 train absolute_loss <loss>=0.7680437081921362\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:57.463] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 2027, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, train rmse <loss>=0.9960322062875406\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, train mse <loss>=0.9920801559620258\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #quality_metric: host=algo-1, epoch=21, train absolute_loss <loss>=0.7864041268224742\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492555.4339113, \"EndTime\": 1615492557.4644914, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2029.4549465179443, \"count\": 1, \"min\": 2029.4549465179443, \"max\": 2029.4549465179443}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492555.4350061, \"EndTime\": 1615492557.4647357, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3317448.0, \"count\": 1, \"min\": 3317448, \"max\": 3317448}, \"Total Batches Seen\": {\"sum\": 16589.0, \"count\": 1, \"min\": 16589, \"max\": 16589}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 23.0, \"count\": 1, \"min\": 23, \"max\": 23}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=74282.97220434285 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, batch=0 train rmse <loss>=1.0112263899075755\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, batch=0 train mse <loss>=1.022578811645508\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:57 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, batch=0 train absolute_loss <loss>=0.8413556671142578\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:58 INFO 140325587183424] Iter[22] Batch [500]#011Speed: 75989.49 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:58 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, batch=500 train rmse <loss>=0.9738312628352889\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:58 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, batch=500 train mse <loss>=0.9483473284753735\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:58 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, batch=500 train absolute_loss <loss>=0.764278439885366\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:55:59.470] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 2002, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, train rmse <loss>=0.9920551985138325\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, train mse <loss>=0.9841735168983197\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #quality_metric: host=algo-1, epoch=22, train absolute_loss <loss>=0.7825760873243094\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492557.464582, \"EndTime\": 1615492559.4707813, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2005.033016204834, \"count\": 1, \"min\": 2005.033016204834, \"max\": 2005.033016204834}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492557.4657185, \"EndTime\": 1615492559.471015, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3468232.0, \"count\": 1, \"min\": 3468232, \"max\": 3468232}, \"Total Batches Seen\": {\"sum\": 17343.0, \"count\": 1, \"min\": 17343, \"max\": 17343}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=75188.02647798543 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, batch=0 train rmse <loss>=1.0070723117598266\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, batch=0 train mse <loss>=1.0141946411132812\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:55:59 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, batch=0 train absolute_loss <loss>=0.8373145294189454\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:00 INFO 140325587183424] Iter[23] Batch [500]#011Speed: 69124.10 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:00 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, batch=500 train rmse <loss>=0.9700517221049572\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:00 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, batch=500 train mse <loss>=0.941000343558793\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:00 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, batch=500 train absolute_loss <loss>=0.7606226881107171\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:56:01.784] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 2310, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, train rmse <loss>=0.9881949598212175\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, train mse <loss>=0.9765292786160578\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #quality_metric: host=algo-1, epoch=23, train absolute_loss <loss>=0.7788535655904512\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492559.470858, \"EndTime\": 1615492561.7847311, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2312.6609325408936, \"count\": 1, \"min\": 2312.6609325408936, \"max\": 2312.6609325408936}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492559.4720397, \"EndTime\": 1615492561.7849784, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3619016.0, \"count\": 1, \"min\": 3619016, \"max\": 3619016}, \"Total Batches Seen\": {\"sum\": 18097.0, \"count\": 1, \"min\": 18097, \"max\": 18097}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=65188.28594476197 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, batch=0 train rmse <loss>=1.0029673990293129\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, batch=0 train mse <loss>=1.005943603515625\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:01 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, batch=0 train absolute_loss <loss>=0.8332862854003906\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:03 INFO 140325587183424] Iter[24] Batch [500]#011Speed: 70973.63 samples/sec\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:03 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, batch=500 train rmse <loss>=0.9663833835450968\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:03 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, batch=500 train mse <loss>=0.9338968439920696\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:03 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, batch=500 train absolute_loss <loss>=0.7570723382489172\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:56:04.085] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 2297, \"num_examples\": 754, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, train rmse <loss>=0.9844421037226123\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, train mse <loss>=0.9691262555818027\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, epoch=24, train absolute_loss <loss>=0.7752314759312637\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, train rmse <loss>=0.9844421037226123\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, train mse <loss>=0.9691262555818027\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, train absolute_loss <loss>=0.7752314759312637\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492561.7848065, \"EndTime\": 1615492564.0870023, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2301.220178604126, \"count\": 1, \"min\": 2301.220178604126, \"max\": 2301.220178604126}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492561.785751, \"EndTime\": 1615492564.0872934, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3769800.0, \"count\": 1, \"min\": 3769800, \"max\": 3769800}, \"Total Batches Seen\": {\"sum\": 18851.0, \"count\": 1, \"min\": 18851, \"max\": 18851}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 754.0, \"count\": 1, \"min\": 754, \"max\": 754}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #throughput_metric: host=algo-1, train throughput=65510.11549070545 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 WARNING 140325587183424] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492564.0870893, \"EndTime\": 1615492564.0925827, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 4.853248596191406, \"count\": 1, \"min\": 4.853248596191406, \"max\": 4.853248596191406}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] Saved checkpoint to \"/tmp/tmp3i1ptnn0/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:56:04.117] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 52861, \"num_examples\": 1, \"num_bytes\": 12800}\u001b[0m\n",
      "\u001b[34m[2021-03-11 19:56:04.330] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 212, \"num_examples\": 47, \"num_bytes\": 592512}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492564.1177115, \"EndTime\": 1615492564.330288, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9258.0, \"count\": 1, \"min\": 9258, \"max\": 9258}, \"Total Batches Seen\": {\"sum\": 47.0, \"count\": 1, \"min\": 47, \"max\": 47}, \"Max Records Seen Between Resets\": {\"sum\": 9258.0, \"count\": 1, \"min\": 9258, \"max\": 9258}, \"Max Batches Seen Between Resets\": {\"sum\": 47.0, \"count\": 1, \"min\": 47, \"max\": 47}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 9258.0, \"count\": 1, \"min\": 9258, \"max\": 9258}, \"Number of Batches Since Last Reset\": {\"sum\": 47.0, \"count\": 1, \"min\": 47, \"max\": 47}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #test_score (algo-1) : ('rmse', 1.1085738901427984)\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #test_score (algo-1) : ('mse', 1.2289360699063374)\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #test_score (algo-1) : ('absolute_loss', 0.8695859775143531)\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, test rmse <loss>=1.1085738901427984\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, test mse <loss>=1.2289360699063374\u001b[0m\n",
      "\u001b[34m[03/11/2021 19:56:04 INFO 140325587183424] #quality_metric: host=algo-1, test absolute_loss <loss>=0.8695859775143531\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615492564.0926907, \"EndTime\": 1615492564.3312697, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 26.034832000732422, \"count\": 1, \"min\": 26.034832000732422, \"max\": 26.034832000732422}, \"totaltime\": {\"sum\": 53111.50097846985, \"count\": 1, \"min\": 53111.50097846985, \"max\": 53111.50097846985}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-11 19:56:22 Uploading - Uploading generated training model\n",
      "2021-03-11 19:56:22 Completed - Training job completed\n",
      "Training seconds: 108\n",
      "Billable seconds: 108\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "\n",
    "output_prefix = 's3://' + bucket + '/sagemaker-fm/model'\n",
    "instance_type= 'ml.m4.xlarge'\n",
    "batch_size = 200\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    image_uris.retrieve(\"factorization-machines\", boto3.Session().region_name),\n",
    "    role, \n",
    "    instance_count=1, # Enter your code here\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_prefix,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "# Use hyperparameter. For feature_dim use the column length of X_train \n",
    "fm.set_hyperparameters(\n",
    "                        feature_dim=customers.shape[0] + products.shape[0], # Enter your code here\n",
    "                        predictor_type='regressor',\n",
    "                        mini_batch_size=batch_size,\n",
    "                        num_factors=64,\n",
    "                        epochs=25,\n",
    "                        clip_gradient=5.0,\n",
    "                        rescale_grad=1.0/batch_size\n",
    ")\n",
    "\n",
    "fm.fit({'train': fm_train_data_path, # Enter your code here, \n",
    "        'test': fm_test_data_path # Enter your code here\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What does changing the `batch_size` and `epochs` do to the final metric?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "# - batch_size: The size of mini-batch used for training. No idea what it does to the final metric.\n",
    "# - epochs: The number of training epochs to run. The bigger, the more optimized the model will be, \n",
    "#           but will take longer to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Check the output of the model. What is the meaning of the metrics used? Is there a difference between the training and testing sets? If yes, what is the meaning of that?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "#  Which metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Congratulations! You have successfully launched an Amazon SageMaker training job. Now what? Well, you need a way to verify that your model is actually predicting coherent values. How do you do this?\n",
    "\n",
    "Start by calculating a naive baseline to approximate how well your model is doing. The simplest estimate would be to assume every user item rating is just the average rating over all ratings. This is basically saying that you have a model that only learned to output the mean value of all reviews.\n",
    "\n",
    "**Note:** You could do better by using each individual video's average; however, in this case, it doesn't really matter because the same conclusions would hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of `star_rating` to get the `naive_guess`. Then, calculate the naive MSE by squaring the naive guess from the test `star_rating` and getting an average.\n",
    "\n",
    "$average(test(star\\_rating) - naive\\_guess)^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive MSE: 17.897176991039853\n"
     ]
    }
   ],
   "source": [
    "naive_guess = np.mean(reduced_df['star_rating']) # Enter your code here\n",
    "print(f'Naive MSE:', np.mean((reduced_df['star_rating'])**2)) # Enter your code here )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate predictions for your test dataset. To this end, you'll need to _deploy_ the model you just trained.\n",
    "\n",
    "**Note:** This will align closely to your CloudWatch output above but may differ slightly due to skipping partial mini-batches in the `eval_net` function.\n",
    "\n",
    "Use `<estimator_name>.deploy` with `initial_instance_count=1, instance_type=ml.m4.xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "fm_predictor = fm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your endpoint is 'InService', evaluate how your model performs on the test set. Compare that test set performance to the performance on the training set. \n",
    "\n",
    "### Key questions to consider:\n",
    "1. How does your model's performance on the test set compare to the training set? What can you deduce from this comparison? \n",
    "\n",
    "2. Are there obvious differences between the outcomes of metrics like accuracy, precision, and recall? If so, why might you be seeing those differences? \n",
    "\n",
    "3. Given your business situation and goals, which metric(s) is most important for you to consider here? Why?\n",
    "\n",
    "4. Is the outcome for the metric(s) you consider most important sufficient for what you need from a business standpoint? If not, what are some things you might change in your next iteration (in the feature engineering section, which is coming up next)? \n",
    "\n",
    "Use the cells below to answer these and other questions. Insert and delete cells where needed.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Record questions to these and other similar questions you might answer in this section in your project presentations. Record key details and decisions you've made in your project presentations.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment process involves creating an instance of the specified size, in this case `ml.m4.xlarge`, with the model you trained and saved on Amazon S3. To get a prediction, you need to pass your data in a serialized form of JSON. The output you get from the inference will be in serialized JSON form as well, so you also need to deserialize it to get the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted content type: application/json\n"
     ]
    }
   ],
   "source": [
    "# Create a serializer function for the predictor\n",
    "import json\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import BaseSerializer\n",
    "\n",
    "class fm_serializer(BaseSerializer):\n",
    "    CONTENT_TYPE='application/json'\n",
    "    def serialize(data):\n",
    "            js = {'instances': []}\n",
    "            for row in data:\n",
    "                js['instances'].append({'features': row.tolist()})\n",
    "            return json.dumps(js)\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = JSONDeserializer()\n",
    "print(f\"Accepted content type: {fm_predictor.content_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how your training set did. Use the endpoint to get predictions from your model.\n",
    "\n",
    "First, look at what a single prediction looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker model containers must respond to requests within 60 seconds. The model itself can have a maximum processing time of 60 seconds before responding to the /invocations. To do that, call the `predict` function for 5 rows at a time and then add those rows to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the X_train data to the deployed predictor \n",
    "ytrain_p = []\n",
    "for i in range(0, 1000, 5):\n",
    "    preds = fm_predictor.predict(X_train[i:i+5].toarray())['predictions'] # Enter your code here\n",
    "    p = [ytrain_p.append(x['score']) for x in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Now that you have inferences, do a sanity check. What are the minimum and maximum values predicted in the inferences? Do those correspond to the minimum and maximum values in the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum rating predicted is:  3.1935508251190186 and the maximum is:  4.853959083557129\n"
     ]
    }
   ],
   "source": [
    "print('The minimum rating predicted is: ', np.min(ytrain_p), # Enter your code here\n",
    "      'and the maximum is: ', np.max(ytrain_p) # Enter your code here\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check your test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "for i in range(0, X_test.shape[0], 5):\n",
    "    preds = fm_predictor.predict(X_test[i:i+5].toarray())['predictions'] # Enter your code here\n",
    "    p = [Y_pred.append(x['score']) for x in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How are the min and max values alike in the predictions? Bonus point if you check the entire distribution (histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.992546558380127, 2.730839252471924)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Y_pred), min(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(Y_pred, kde=False, bins=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculate the mean squared error for the test set and see how much of an improvement it is from the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.2289360770982505\n"
     ]
    }
   ],
   "source": [
    "print('MSE:', np.mean((Y_test - Y_pred)**2) )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For recommender systems, subjective accuracy also matters. Get some recommendations for a random user to see if they make intuitive sense.\n",
    "\n",
    "Try using user number 200, and see what they have watched and rated highly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13782</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00APE1NZW</td>\n",
       "      <td>Justified Season 4</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00821OX98</td>\n",
       "      <td>Falling Skies Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75399</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00HB8UWCU</td>\n",
       "      <td>Escape Plan</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98168</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B0099RFVXQ</td>\n",
       "      <td>Marvel's The Avengers</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89552</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00IMYZL2S</td>\n",
       "      <td>Thor: The Dark World</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30831</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B008BQ8YHQ</td>\n",
       "      <td>Suits Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60712</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B008BQG3RE</td>\n",
       "      <td>Falling Skies Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101821</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00AA7O22U</td>\n",
       "      <td>The Expendables 2</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118368</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00AMBLXP8</td>\n",
       "      <td>Ted</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20486</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B007JF8BS2</td>\n",
       "      <td>Duck Dynasty Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22624</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00COGXGNQ</td>\n",
       "      <td>Safe Haven</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101492</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B009PI8DFC</td>\n",
       "      <td>2016 Obama's America</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96089</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B004AH3I62</td>\n",
       "      <td>Ancient Aliens Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69138</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B004H5XUPQ</td>\n",
       "      <td>Ancient Aliens Season 2</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79785</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B009RURGHO</td>\n",
       "      <td>The Men Who Built America Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134010</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00748O13S</td>\n",
       "      <td>Alias Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>3990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148496</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B002STKILW</td>\n",
       "      <td>Tosh.0 Season 1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>4829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37176</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00I9AHY2K</td>\n",
       "      <td>Vikings Season 2</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41261</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00CDZFRAI</td>\n",
       "      <td>Alpha House Season 1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58434</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00KG2SF5O</td>\n",
       "      <td>3 Days to Kill</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89993</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00AHSIQWE</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00C0OQ1Z2</td>\n",
       "      <td>Defiance Season 1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13367</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00IK590UI</td>\n",
       "      <td>Robocop (2014)</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95884</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00A0SKFLG</td>\n",
       "      <td>Total Recall</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101946</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00KATY250</td>\n",
       "      <td>Her (2013)</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114990</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00RC7JCR6</td>\n",
       "      <td>Let's Kill Ward's Wife</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145586</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00LM4ST9W</td>\n",
       "      <td>Bad Words</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69107</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B005F2BB3I</td>\n",
       "      <td>Ancient Aliens Season 3</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71117</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B002QYUVY8</td>\n",
       "      <td>Kiss Kiss Bang Bang</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>1586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135715</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00AC8ZIXO</td>\n",
       "      <td>Halo 4: Forward Unto Dawn</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38863</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B003AZCYCE</td>\n",
       "      <td>Justified Season 1</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12360</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00DAHSY58</td>\n",
       "      <td>Under The Dome, Season 1</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12681</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00VFTA70I</td>\n",
       "      <td>The Imitation Game</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22181</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00PJW38BW</td>\n",
       "      <td>The Captive</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60242</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00H8AJTB6</td>\n",
       "      <td>Hours</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108540</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00AE2J5HI</td>\n",
       "      <td>Lawless</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77956</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B009B0JR2C</td>\n",
       "      <td>The Amazing Spider-Man</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35069</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00B7G3NFU</td>\n",
       "      <td>The Sessions</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128849</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00BSBU8P2</td>\n",
       "      <td>Killing Them Softly</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129463</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B009YR6YN2</td>\n",
       "      <td>The Campaign (2012)</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>2733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148485</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00U7ZPIP0</td>\n",
       "      <td>The Cobbler</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148505</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B004IY9DQQ</td>\n",
       "      <td>Tosh.0 Season 3</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>5866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33964</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00AMBH70S</td>\n",
       "      <td>The Bourne Legacy</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82515</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00C1BU7V8</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65852</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00CL68NOG</td>\n",
       "      <td>The Guilt Trip</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25981</th>\n",
       "      <td>35077976</td>\n",
       "      <td>B00DKS3QZU</td>\n",
       "      <td>Drunk History Season 1</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer_id  product_id                       product_title  \\\n",
       "13782      35077976  B00APE1NZW                  Justified Season 4   \n",
       "3910       35077976  B00821OX98              Falling Skies Season 1   \n",
       "75399      35077976  B00HB8UWCU                         Escape Plan   \n",
       "98168      35077976  B0099RFVXQ               Marvel's The Avengers   \n",
       "89552      35077976  B00IMYZL2S                Thor: The Dark World   \n",
       "30831      35077976  B008BQ8YHQ                      Suits Season 2   \n",
       "60712      35077976  B008BQG3RE              Falling Skies Season 2   \n",
       "101821     35077976  B00AA7O22U                   The Expendables 2   \n",
       "118368     35077976  B00AMBLXP8                                 Ted   \n",
       "20486      35077976  B007JF8BS2               Duck Dynasty Season 1   \n",
       "22624      35077976  B00COGXGNQ                          Safe Haven   \n",
       "101492     35077976  B009PI8DFC                2016 Obama's America   \n",
       "96089      35077976  B004AH3I62             Ancient Aliens Season 1   \n",
       "69138      35077976  B004H5XUPQ             Ancient Aliens Season 2   \n",
       "79785      35077976  B009RURGHO  The Men Who Built America Season 1   \n",
       "134010     35077976  B00748O13S                      Alias Season 1   \n",
       "148496     35077976  B002STKILW                     Tosh.0 Season 1   \n",
       "37176      35077976  B00I9AHY2K                    Vikings Season 2   \n",
       "41261      35077976  B00CDZFRAI                Alpha House Season 1   \n",
       "58434      35077976  B00KG2SF5O                      3 Days to Kill   \n",
       "89993      35077976  B00AHSIQWE               The Dark Knight Rises   \n",
       "7868       35077976  B00C0OQ1Z2                   Defiance Season 1   \n",
       "13367      35077976  B00IK590UI                      Robocop (2014)   \n",
       "95884      35077976  B00A0SKFLG                        Total Recall   \n",
       "101946     35077976  B00KATY250                          Her (2013)   \n",
       "114990     35077976  B00RC7JCR6              Let's Kill Ward's Wife   \n",
       "145586     35077976  B00LM4ST9W                           Bad Words   \n",
       "69107      35077976  B005F2BB3I             Ancient Aliens Season 3   \n",
       "71117      35077976  B002QYUVY8                 Kiss Kiss Bang Bang   \n",
       "135715     35077976  B00AC8ZIXO           Halo 4: Forward Unto Dawn   \n",
       "38863      35077976  B003AZCYCE                  Justified Season 1   \n",
       "12360      35077976  B00DAHSY58            Under The Dome, Season 1   \n",
       "12681      35077976  B00VFTA70I                  The Imitation Game   \n",
       "22181      35077976  B00PJW38BW                         The Captive   \n",
       "60242      35077976  B00H8AJTB6                               Hours   \n",
       "108540     35077976  B00AE2J5HI                             Lawless   \n",
       "77956      35077976  B009B0JR2C              The Amazing Spider-Man   \n",
       "35069      35077976  B00B7G3NFU                        The Sessions   \n",
       "128849     35077976  B00BSBU8P2                 Killing Them Softly   \n",
       "129463     35077976  B009YR6YN2                 The Campaign (2012)   \n",
       "148485     35077976  B00U7ZPIP0                         The Cobbler   \n",
       "148505     35077976  B004IY9DQQ                     Tosh.0 Season 3   \n",
       "33964      35077976  B00AMBH70S                   The Bourne Legacy   \n",
       "82515      35077976  B00C1BU7V8                             Lincoln   \n",
       "65852      35077976  B00CL68NOG                      The Guilt Trip   \n",
       "25981      35077976  B00DKS3QZU              Drunk History Season 1   \n",
       "\n",
       "        star_rating  user  item  \n",
       "13782             5   200     6  \n",
       "3910              5   200    47  \n",
       "75399             5   200   118  \n",
       "98168             5   200   140  \n",
       "89552             5   200   154  \n",
       "30831             5   200   155  \n",
       "60712             5   200   233  \n",
       "101821            5   200   265  \n",
       "118368            5   200   280  \n",
       "20486             5   200   337  \n",
       "22624             5   200   581  \n",
       "101492            5   200   584  \n",
       "96089             5   200   871  \n",
       "69138             5   200  1174  \n",
       "79785             5   200  1223  \n",
       "134010            5   200  3990  \n",
       "148496            5   200  4829  \n",
       "37176             4   200    52  \n",
       "41261             4   200    56  \n",
       "58434             4   200    68  \n",
       "89993             4   200    86  \n",
       "7868              4   200    92  \n",
       "13367             4   200   126  \n",
       "95884             4   200   134  \n",
       "101946            4   200   286  \n",
       "114990            4   200   671  \n",
       "145586            4   200  1260  \n",
       "69107             4   200  1292  \n",
       "71117             4   200  1586  \n",
       "135715            4   200  2649  \n",
       "38863             3   200     1  \n",
       "12360             3   200     5  \n",
       "12681             3   200    76  \n",
       "22181             3   200    77  \n",
       "60242             3   200   373  \n",
       "108540            3   200   501  \n",
       "77956             3   200   538  \n",
       "35069             3   200   790  \n",
       "128849            3   200  1149  \n",
       "129463            3   200  2733  \n",
       "148485            3   200  4135  \n",
       "148505            3   200  5866  \n",
       "33964             1   200    95  \n",
       "82515             1   200   105  \n",
       "65852             1   200   530  \n",
       "25981             1   200   726  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df[reduced_df['user'] == 200].sort_values(\n",
    "    ['star_rating', 'item'], ascending=[False, True]) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this user likes to watch comedies, romance, and light-hearted movies and dislikes drama and fantasy movies. Let's see how your model predicts movie ratings for this user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predictions(user_id, number_movies, columns):\n",
    "    # Create the sparse matrix similar to the one for training data\n",
    "    X = lil_matrix((number_movies, columns)).astype('float32')# Enter your code here\n",
    "    movie_index_start = columns - number_movies\n",
    "\n",
    "    # Fill out the matrix. Each row will be the same user with every possible movie.\n",
    "    for row in range(number_movies):\n",
    "        X[row, user_id - 1] = 1 # Enter your code here\n",
    "        X[row, movie_index_start + row] = 1 # Enter your code here\n",
    "\n",
    "    return X\n",
    "\n",
    "user_200 = prepare_predictions(200, products.shape[0], customers.shape[0] + products.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a list of all the ratings that the model would predict for user 200 for all movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_200 = []\n",
    "for i in range(0, user_200.shape[0], 5):\n",
    "    preds = fm_predictor.predict(user_200[i:i+5].toarray())['predictions']\n",
    "    p = [pred_200.append(x['score']) for x in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through and predict user 200's ratings for every common video in the catalog to see which ones to recommend or not recommend. \n",
    "\n",
    "Create a new dataframe `titles` by using the `reduced_df` dataframe to group by the items. Use the `product_title` column and create another column `score` and add the values from `pred_200` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = reduced_df.groupby('item')['product_title'].first().reset_index()\n",
    "titles['score'] = pred_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What products got the highest score?  \n",
    "\n",
    "**Hint**: Use the `sort_values` function to sort columns `score` and `item` and use parameter `asecnding=[False,True]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>product_title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>209</td>\n",
       "      <td>The Hundred-Foot Journey (Theatrical)</td>\n",
       "      <td>4.539595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>262</td>\n",
       "      <td>The Impossible</td>\n",
       "      <td>4.537935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>3141</td>\n",
       "      <td>The Score</td>\n",
       "      <td>4.532291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>809</td>\n",
       "      <td>The Hundred-Foot Journey (Theatrical)</td>\n",
       "      <td>4.531047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>4292</td>\n",
       "      <td>Toy Story 2</td>\n",
       "      <td>4.530080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>803</td>\n",
       "      <td>The Warriors Way</td>\n",
       "      <td>3.980348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>5076</td>\n",
       "      <td>Drunk History Season 1</td>\n",
       "      <td>3.978236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>2302</td>\n",
       "      <td>Dumb And Dumber To</td>\n",
       "      <td>3.977507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>4386</td>\n",
       "      <td>Life After Beth</td>\n",
       "      <td>3.972282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>407</td>\n",
       "      <td>Under the Skin</td>\n",
       "      <td>3.950845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6583 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item                          product_title     score\n",
       "209    209  The Hundred-Foot Journey (Theatrical)  4.539595\n",
       "262    262                         The Impossible  4.537935\n",
       "3141  3141                              The Score  4.532291\n",
       "809    809  The Hundred-Foot Journey (Theatrical)  4.531047\n",
       "4292  4292                            Toy Story 2  4.530080\n",
       "...    ...                                    ...       ...\n",
       "803    803                       The Warriors Way  3.980348\n",
       "5076  5076                 Drunk History Season 1  3.978236\n",
       "2302  2302                     Dumb And Dumber To  3.977507\n",
       "4386  4386                        Life After Beth  3.972282\n",
       "407    407                         Under the Skin  3.950845\n",
       "\n",
       "[6583 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.sort_values(['score', 'item'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What can you conclude from the highly rated and lowest rated shows for the user? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if your recommendations have correlations with other users. Try user 201. Perform the same operations as you did for user 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_201 = prepare_predictions(201, products.shape[0], customers.shape[0] + products.shape[0])\n",
    "\n",
    "pred_201 = []\n",
    "for i in range(0, user_201.shape[0], 5):\n",
    "    preds = fm_predictor.predict(user_201[i:i+5].toarray())['predictions']\n",
    "    p = [pred_201.append(x['score']) for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f583f8ba5f8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pred_200, pred_201)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What can you conclude from the scatter plot between the two users?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the endpoint you created for inference because you won't be using it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(fm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> End of Lab 3 </span>\n",
    "\n",
    "Save the project file to your local computer. Follow these steps:\n",
    "\n",
    "1. At the top of the page, click the **File** menu. \n",
    "\n",
    "1. Select **Download as**, and click **Notebook(.ipynb)**.  \n",
    "\n",
    "This downloads the current notebook to the default download folder on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration II\n",
    "\n",
    "# Step 4: Feature engineering\n",
    "\n",
    "You've now gone through one iteration of training and evaluating your model. Given that the outcome you reached for your model the first time probably wasn't sufficient for solving your business problem, what are some things you could change about your data to possibly improve model performance?\n",
    "\n",
    "### Key questions to consider:\n",
    "1. How might changing the machine learning problem help your dataset? You tried to use regression to solve the problem; can classification help?\n",
    "2. What do you need to do to change the machine learning problem to a machine learning classification problem? Write down the new problem statement for classification.\n",
    "\n",
    "#### <span style=\"color: blue;\">Project presentation: Record key decisions and methods you use in this section in your project presentations, as well as any new performance metrics you obtain after evaluating your model again.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the training datasets to have a binary output depending on the rating they get. You will consider recommending something to a user when the rating is 5 stars, and you will save again as a protobuf format in Amazon S3. Do the following:  \n",
    "\n",
    "1. Use the `loadDataset` function with the option `regression=False` to create your training datasets.   \n",
    "2. Write the dataset as a protobuf format.  \n",
    "3. Retrain the model using `predictor_type='binary_classifier'`.   \n",
    "4. Deploy your model to an endpoint and evaluate the model, similar to how you did before on the test set.   \n",
    "5. Inspect how you did on the test set using a confusion matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train = loadDataset(train_df, customers.shape[0], products.shape[0])  # Enter your code here\n",
    "X_train_class, Y_train_class = loadDataset(train_df, customers.shape[0], products.shape[0], regressor=False)\n",
    "X_test_class, Y_test_class = loadDataset(test_df, customers.shape[0], products.shape[0],  regressor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data S3 path:  s3://qls-4019912-5ead0778b062168c-labbucket-1h2g0kyyebu6d/sagemaker-fm/train_class\n",
      "Test data S3 path:  s3://qls-4019912-5ead0778b062168c-labbucket-1h2g0kyyebu6d/sagemaker-fm/test_class\n"
     ]
    }
   ],
   "source": [
    "# Write dataset as a protobuf\n",
    "fm_train_data_path = writeDatasetToProtobuf(X_train_class, bucket, prefix, 'train_class', \"sparse\", Y_train_class) # Enter your code here    \n",
    "fm_test_data_path  = writeDatasetToProtobuf(X_test_class, bucket, prefix, 'test_class', \"sparse\", Y_test_class) # Enter your code here    \n",
    "  \n",
    "print(\"Training data S3 path: \", fm_train_data_path)\n",
    "print(\"Test data S3 path: \", fm_test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code\n",
    "\n",
    "```\n",
    "fm_train_data_path = writeDatasetToProtobuf(X_train_class, bucket, prefix, 'train_class', \"sparse\", Y_train_class)    \n",
    "fm_test_data_path  = writeDatasetToProtobuf(X_test_class, bucket, prefix, 'test_class', \"sparse\", Y_test_class) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, retrain the model, changing from regression to binary classification. Use the same code and settings that you did when you trained your model previously, but change the `predictor_type='binary_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 20:41:23 Starting - Starting the training job...\n",
      "2021-03-11 20:41:46 Starting - Launching requested ML instancesProfilerReport-1615495283: InProgress\n",
      ".........\n",
      "2021-03-11 20:43:07 Starting - Preparing the instances for training......\n",
      "2021-03-11 20:44:17 Downloading - Downloading input data...\n",
      "2021-03-11 20:44:47 Training - Downloading the training image...\n",
      "2021-03-11 20:45:07 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '15841', 'predictor_type': 'binary_classifier', 'rescale_grad': '0.001953125', 'clip_gradient': '5.0', 'num_factors': '128', 'epochs': '25', 'mini_batch_size': '512'}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] Final configuration: {'epochs': '25', 'mini_batch_size': '512', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '15841', 'predictor_type': 'binary_classifier', 'rescale_grad': '0.001953125', 'clip_gradient': '5.0', 'num_factors': '128'}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 WARNING 139973900457792] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:09.698] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:09.703] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 32768}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] nvidia-smi took: 0.0252685546875 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495509.6940095, \"EndTime\": 1615495509.7403219, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 39.25609588623047, \"count\": 1, \"min\": 39.25609588623047, \"max\": 39.25609588623047}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495509.7404742, \"EndTime\": 1615495509.7405171, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 512.0, \"count\": 1, \"min\": 512, \"max\": 512}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 512.0, \"count\": 1, \"min\": 512, \"max\": 512}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[20:45:09] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204326.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[20:45:09] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204326.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_accuracy <score>=0.392578125\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_cross_entropy <loss>=0.6950399875640869\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:09 INFO 139973900457792] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_f_1.000 <score>=0.5193199381761978\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:12.080] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2216, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #quality_metric: host=algo-1, epoch=0, train binary_classification_accuracy <score>=0.5226959745762711\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #quality_metric: host=algo-1, epoch=0, train binary_classification_cross_entropy <loss>=0.6915829735287166\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #quality_metric: host=algo-1, epoch=0, train binary_f_1.000 <score>=0.4858430684525083\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495509.7404103, \"EndTime\": 1615495512.0807378, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"update.time\": {\"sum\": 2339.933156967163, \"count\": 1, \"min\": 2339.933156967163, \"max\": 2339.933156967163}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495509.740769, \"EndTime\": 1615495512.0810695, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 151296.0, \"count\": 1, \"min\": 151296, \"max\": 151296}, \"Total Batches Seen\": {\"sum\": 296.0, \"count\": 1, \"min\": 296, \"max\": 296}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=64425.07165539611 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_accuracy <score>=0.275390625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_cross_entropy <loss>=0.7188963890075684\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:12 INFO 139973900457792] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_f_1.000 <score>=0.0\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:14.249] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2163, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #quality_metric: host=algo-1, epoch=1, train binary_classification_accuracy <score>=0.5557203389830508\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #quality_metric: host=algo-1, epoch=1, train binary_classification_cross_entropy <loss>=0.6838557788881204\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #quality_metric: host=algo-1, epoch=1, train binary_f_1.000 <score>=0.5125168901739143\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495512.0808492, \"EndTime\": 1615495514.249949, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2168.496608734131, \"count\": 1, \"min\": 2168.496608734131, \"max\": 2168.496608734131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495512.0814192, \"EndTime\": 1615495514.2502067, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 302080.0, \"count\": 1, \"min\": 302080, \"max\": 302080}, \"Total Batches Seen\": {\"sum\": 591.0, \"count\": 1, \"min\": 591, \"max\": 591}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=69520.52694875126 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_accuracy <score>=0.28125\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_cross_entropy <loss>=0.7107709646224976\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:14 INFO 139973900457792] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_f_1.000 <score>=0.016042780748663103\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:16.089] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 1837, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #quality_metric: host=algo-1, epoch=2, train binary_classification_accuracy <score>=0.5907375529661016\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #quality_metric: host=algo-1, epoch=2, train binary_classification_cross_entropy <loss>=0.6756618368423591\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #quality_metric: host=algo-1, epoch=2, train binary_f_1.000 <score>=0.5515159869695496\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495514.2500129, \"EndTime\": 1615495516.089939, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1839.430809020996, \"count\": 1, \"min\": 1839.430809020996, \"max\": 1839.430809020996}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495514.2504735, \"EndTime\": 1615495516.0902505, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 452864.0, \"count\": 1, \"min\": 452864, \"max\": 452864}, \"Total Batches Seen\": {\"sum\": 886.0, \"count\": 1, \"min\": 886, \"max\": 886}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81950.4315041814 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_accuracy <score>=0.322265625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_cross_entropy <loss>=0.7028709650039673\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:16 INFO 139973900457792] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_f_1.000 <score>=0.13032581453634084\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:18.059] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1967, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #quality_metric: host=algo-1, epoch=3, train binary_classification_accuracy <score>=0.6204912605932204\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #quality_metric: host=algo-1, epoch=3, train binary_classification_cross_entropy <loss>=0.6681656007039345\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #quality_metric: host=algo-1, epoch=3, train binary_f_1.000 <score>=0.5866641668890027\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495516.090032, \"EndTime\": 1615495518.0599787, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1969.3384170532227, \"count\": 1, \"min\": 1969.3384170532227, \"max\": 1969.3384170532227}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495516.0906005, \"EndTime\": 1615495518.0602813, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 603648.0, \"count\": 1, \"min\": 603648, \"max\": 603648}, \"Total Batches Seen\": {\"sum\": 1181.0, \"count\": 1, \"min\": 1181, \"max\": 1181}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=76546.00154971919 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_accuracy <score>=0.44140625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_cross_entropy <loss>=0.6954574584960938\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:18 INFO 139973900457792] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_f_1.000 <score>=0.39148936170212767\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:19.907] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 1844, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #quality_metric: host=algo-1, epoch=4, train binary_classification_accuracy <score>=0.6447497351694915\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #quality_metric: host=algo-1, epoch=4, train binary_classification_cross_entropy <loss>=0.6614341438826868\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #quality_metric: host=algo-1, epoch=4, train binary_f_1.000 <score>=0.6161297476731126\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495518.0600767, \"EndTime\": 1615495519.9079607, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1847.2836017608643, \"count\": 1, \"min\": 1847.2836017608643, \"max\": 1847.2836017608643}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495518.0606387, \"EndTime\": 1615495519.9082553, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 754432.0, \"count\": 1, \"min\": 754432, \"max\": 754432}, \"Total Batches Seen\": {\"sum\": 1476.0, \"count\": 1, \"min\": 1476, \"max\": 1476}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81602.57810259929 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_accuracy <score>=0.5390625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_cross_entropy <loss>=0.6885565519332886\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:19 INFO 139973900457792] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_f_1.000 <score>=0.5597014925373134\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:21.792] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 1881, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #quality_metric: host=algo-1, epoch=5, train binary_classification_accuracy <score>=0.6638307733050848\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #quality_metric: host=algo-1, epoch=5, train binary_classification_cross_entropy <loss>=0.6553745801165952\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #quality_metric: host=algo-1, epoch=5, train binary_f_1.000 <score>=0.6395996734925649\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495519.9080563, \"EndTime\": 1615495521.793094, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1884.4854831695557, \"count\": 1, \"min\": 1884.4854831695557, \"max\": 1884.4854831695557}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495519.9085784, \"EndTime\": 1615495521.793342, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 905216.0, \"count\": 1, \"min\": 905216, \"max\": 905216}, \"Total Batches Seen\": {\"sum\": 1771.0, \"count\": 1, \"min\": 1771, \"max\": 1771}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=79996.44997941378 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_accuracy <score>=0.591796875\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_cross_entropy <loss>=0.6821247935295105\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:21 INFO 139973900457792] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_f_1.000 <score>=0.6339754816112084\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:23.701] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1905, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #quality_metric: host=algo-1, epoch=6, train binary_classification_accuracy <score>=0.6783898305084746\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #quality_metric: host=algo-1, epoch=6, train binary_classification_cross_entropy <loss>=0.6498801879963633\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #quality_metric: host=algo-1, epoch=6, train binary_f_1.000 <score>=0.6575585821842483\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495521.7931786, \"EndTime\": 1615495523.7018921, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1908.238172531128, \"count\": 1, \"min\": 1908.238172531128, \"max\": 1908.238172531128}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495521.7936125, \"EndTime\": 1615495523.702247, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1056000.0, \"count\": 1, \"min\": 1056000, \"max\": 1056000}, \"Total Batches Seen\": {\"sum\": 2066.0, \"count\": 1, \"min\": 2066, \"max\": 2066}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=78994.19632851578 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_accuracy <score>=0.62890625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_cross_entropy <loss>=0.6761040687561035\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:23 INFO 139973900457792] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_f_1.000 <score>=0.6790540540540541\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:25.579] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 1875, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #quality_metric: host=algo-1, epoch=7, train binary_classification_accuracy <score>=0.6890889830508474\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #quality_metric: host=algo-1, epoch=7, train binary_classification_cross_entropy <loss>=0.6448573328680911\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #quality_metric: host=algo-1, epoch=7, train binary_f_1.000 <score>=0.670636423571659\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495523.701998, \"EndTime\": 1615495525.5806932, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1878.0436515808105, \"count\": 1, \"min\": 1878.0436515808105, \"max\": 1878.0436515808105}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495523.702604, \"EndTime\": 1615495525.5810006, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1206784.0, \"count\": 1, \"min\": 1206784, \"max\": 1206784}, \"Total Batches Seen\": {\"sum\": 2361.0, \"count\": 1, \"min\": 2361, \"max\": 2361}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=80265.95674145824 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_accuracy <score>=0.654296875\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_cross_entropy <loss>=0.6704392433166504\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:25 INFO 139973900457792] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_f_1.000 <score>=0.7084019769357496\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:27.423] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 1840, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #quality_metric: host=algo-1, epoch=8, train binary_classification_accuracy <score>=0.6986493644067797\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #quality_metric: host=algo-1, epoch=8, train binary_classification_cross_entropy <loss>=0.6402286206261586\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #quality_metric: host=algo-1, epoch=8, train binary_f_1.000 <score>=0.6819153842928425\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495525.5807965, \"EndTime\": 1615495527.4243002, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1842.9365158081055, \"count\": 1, \"min\": 1842.9365158081055, \"max\": 1842.9365158081055}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495525.5813293, \"EndTime\": 1615495527.4245386, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1357568.0, \"count\": 1, \"min\": 1357568, \"max\": 1357568}, \"Total Batches Seen\": {\"sum\": 2656.0, \"count\": 1, \"min\": 2656, \"max\": 2656}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81798.37270015856 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_accuracy <score>=0.671875\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_cross_entropy <loss>=0.6650822162628174\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:27 INFO 139973900457792] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_f_1.000 <score>=0.7290322580645161\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:29.333] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1906, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #quality_metric: host=algo-1, epoch=9, train binary_classification_accuracy <score>=0.7059917902542373\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #quality_metric: host=algo-1, epoch=9, train binary_classification_cross_entropy <loss>=0.6359310633045132\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #quality_metric: host=algo-1, epoch=9, train binary_f_1.000 <score>=0.6905025752538664\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495527.424379, \"EndTime\": 1615495529.3341918, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1909.3427658081055, \"count\": 1, \"min\": 1909.3427658081055, \"max\": 1909.3427658081055}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495527.4248161, \"EndTime\": 1615495529.3344443, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1508352.0, \"count\": 1, \"min\": 1508352, \"max\": 1508352}, \"Total Batches Seen\": {\"sum\": 2951.0, \"count\": 1, \"min\": 2951, \"max\": 2951}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=78954.42366188546 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #quality_metric: host=algo-1, epoch=10, batch=0 train binary_classification_accuracy <score>=0.677734375\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #quality_metric: host=algo-1, epoch=10, batch=0 train binary_classification_cross_entropy <loss>=0.6599928140640259\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:29 INFO 139973900457792] #quality_metric: host=algo-1, epoch=10, batch=0 train binary_f_1.000 <score>=0.7351524879614767\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:31.248] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 1911, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #quality_metric: host=algo-1, epoch=10, train binary_classification_accuracy <score>=0.713089247881356\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #quality_metric: host=algo-1, epoch=10, train binary_classification_cross_entropy <loss>=0.6319134423288248\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #quality_metric: host=algo-1, epoch=10, train binary_f_1.000 <score>=0.6986334712611704\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495529.3342817, \"EndTime\": 1615495531.2490463, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1914.3033027648926, \"count\": 1, \"min\": 1914.3033027648926, \"max\": 1914.3033027648926}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495529.3347096, \"EndTime\": 1615495531.2493305, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1659136.0, \"count\": 1, \"min\": 1659136, \"max\": 1659136}, \"Total Batches Seen\": {\"sum\": 3246.0, \"count\": 1, \"min\": 3246, \"max\": 3246}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=78748.75568152245 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #quality_metric: host=algo-1, epoch=11, batch=0 train binary_classification_accuracy <score>=0.69140625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #quality_metric: host=algo-1, epoch=11, batch=0 train binary_classification_cross_entropy <loss>=0.6551371216773987\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:31 INFO 139973900457792] #quality_metric: host=algo-1, epoch=11, batch=0 train binary_f_1.000 <score>=0.75\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:33.144] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 1893, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #quality_metric: host=algo-1, epoch=11, train binary_classification_accuracy <score>=0.7189221398305085\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #quality_metric: host=algo-1, epoch=11, train binary_classification_cross_entropy <loss>=0.6281340138386872\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #quality_metric: host=algo-1, epoch=11, train binary_f_1.000 <score>=0.7051068322636215\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495531.2491279, \"EndTime\": 1615495533.1453571, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1895.7006931304932, \"count\": 1, \"min\": 1895.7006931304932, \"max\": 1895.7006931304932}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495531.249629, \"EndTime\": 1615495533.1455324, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1809920.0, \"count\": 1, \"min\": 1809920, \"max\": 1809920}, \"Total Batches Seen\": {\"sum\": 3541.0, \"count\": 1, \"min\": 3541, \"max\": 3541}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=79526.96998332338 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #quality_metric: host=algo-1, epoch=12, batch=0 train binary_classification_accuracy <score>=0.6953125\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #quality_metric: host=algo-1, epoch=12, batch=0 train binary_classification_cross_entropy <loss>=0.6504867076873779\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:33 INFO 139973900457792] #quality_metric: host=algo-1, epoch=12, batch=0 train binary_f_1.000 <score>=0.7539432176656151\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:34.967] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1820, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #quality_metric: host=algo-1, epoch=12, train binary_classification_accuracy <score>=0.724318061440678\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #quality_metric: host=algo-1, epoch=12, train binary_classification_cross_entropy <loss>=0.6245585199129784\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #quality_metric: host=algo-1, epoch=12, train binary_f_1.000 <score>=0.7112513435733852\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495533.1454167, \"EndTime\": 1615495534.9682553, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1822.4594593048096, \"count\": 1, \"min\": 1822.4594593048096, \"max\": 1822.4594593048096}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495533.1457648, \"EndTime\": 1615495534.9685235, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1960704.0, \"count\": 1, \"min\": 1960704, \"max\": 1960704}, \"Total Batches Seen\": {\"sum\": 3836.0, \"count\": 1, \"min\": 3836, \"max\": 3836}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=82716.71025095928 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #quality_metric: host=algo-1, epoch=13, batch=0 train binary_classification_accuracy <score>=0.70703125\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #quality_metric: host=algo-1, epoch=13, batch=0 train binary_classification_cross_entropy <loss>=0.646017849445343\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:34 INFO 139973900457792] #quality_metric: host=algo-1, epoch=13, batch=0 train binary_f_1.000 <score>=0.765625\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:36.808] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 1837, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #quality_metric: host=algo-1, epoch=13, train binary_classification_accuracy <score>=0.7295021186440678\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #quality_metric: host=algo-1, epoch=13, train binary_classification_cross_entropy <loss>=0.6211587241140463\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #quality_metric: host=algo-1, epoch=13, train binary_f_1.000 <score>=0.7169853144915489\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495534.9683626, \"EndTime\": 1615495536.8092003, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1840.3806686401367, \"count\": 1, \"min\": 1840.3806686401367, \"max\": 1840.3806686401367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495534.9687874, \"EndTime\": 1615495536.8094478, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2111488.0, \"count\": 1, \"min\": 2111488, \"max\": 2111488}, \"Total Batches Seen\": {\"sum\": 4131.0, \"count\": 1, \"min\": 4131, \"max\": 4131}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81912.72984729872 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #quality_metric: host=algo-1, epoch=14, batch=0 train binary_classification_accuracy <score>=0.71484375\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #quality_metric: host=algo-1, epoch=14, batch=0 train binary_classification_cross_entropy <loss>=0.6417109966278076\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:36 INFO 139973900457792] #quality_metric: host=algo-1, epoch=14, batch=0 train binary_f_1.000 <score>=0.7732919254658385\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:38.738] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 1926, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #quality_metric: host=algo-1, epoch=14, train binary_classification_accuracy <score>=0.7338916843220339\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #quality_metric: host=algo-1, epoch=14, train binary_classification_cross_entropy <loss>=0.6179112074738842\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #quality_metric: host=algo-1, epoch=14, train binary_f_1.000 <score>=0.7218034704485835\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495536.8092859, \"EndTime\": 1615495538.7390833, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1929.3417930603027, \"count\": 1, \"min\": 1929.3417930603027, \"max\": 1929.3417930603027}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495536.8097045, \"EndTime\": 1615495538.7393796, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2262272.0, \"count\": 1, \"min\": 2262272, \"max\": 2262272}, \"Total Batches Seen\": {\"sum\": 4426.0, \"count\": 1, \"min\": 4426, \"max\": 4426}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=78133.38129342106 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #quality_metric: host=algo-1, epoch=15, batch=0 train binary_classification_accuracy <score>=0.7265625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #quality_metric: host=algo-1, epoch=15, batch=0 train binary_classification_cross_entropy <loss>=0.6375496983528137\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:38 INFO 139973900457792] #quality_metric: host=algo-1, epoch=15, batch=0 train binary_f_1.000 <score>=0.7846153846153846\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:40.703] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1962, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #quality_metric: host=algo-1, epoch=15, train binary_classification_accuracy <score>=0.7378840042372882\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #quality_metric: host=algo-1, epoch=15, train binary_classification_cross_entropy <loss>=0.6147964249222966\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #quality_metric: host=algo-1, epoch=15, train binary_f_1.000 <score>=0.7261496320070832\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495538.7391734, \"EndTime\": 1615495540.7043025, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1964.5447731018066, \"count\": 1, \"min\": 1964.5447731018066, \"max\": 1964.5447731018066}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495538.7397227, \"EndTime\": 1615495540.704552, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2413056.0, \"count\": 1, \"min\": 2413056, \"max\": 2413056}, \"Total Batches Seen\": {\"sum\": 4721.0, \"count\": 1, \"min\": 4721, \"max\": 4721}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=76735.7013515509 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #quality_metric: host=algo-1, epoch=16, batch=0 train binary_classification_accuracy <score>=0.732421875\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #quality_metric: host=algo-1, epoch=16, batch=0 train binary_classification_cross_entropy <loss>=0.6335203647613525\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:40 INFO 139973900457792] #quality_metric: host=algo-1, epoch=16, batch=0 train binary_f_1.000 <score>=0.7895545314900153\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:42.543] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 1836, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #quality_metric: host=algo-1, epoch=16, train binary_classification_accuracy <score>=0.7416313559322034\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #quality_metric: host=algo-1, epoch=16, train binary_classification_cross_entropy <loss>=0.6117979783122822\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #quality_metric: host=algo-1, epoch=16, train binary_f_1.000 <score>=0.7302550632473906\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495540.704389, \"EndTime\": 1615495542.5440383, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1839.1785621643066, \"count\": 1, \"min\": 1839.1785621643066, \"max\": 1839.1785621643066}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495540.7048237, \"EndTime\": 1615495542.5443358, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2563840.0, \"count\": 1, \"min\": 2563840, \"max\": 2563840}, \"Total Batches Seen\": {\"sum\": 5016.0, \"count\": 1, \"min\": 5016, \"max\": 5016}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81962.95332496554 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #quality_metric: host=algo-1, epoch=17, batch=0 train binary_classification_accuracy <score>=0.734375\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #quality_metric: host=algo-1, epoch=17, batch=0 train binary_classification_cross_entropy <loss>=0.6296113729476929\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:42 INFO 139973900457792] #quality_metric: host=algo-1, epoch=17, batch=0 train binary_f_1.000 <score>=0.7920489296636085\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:44.411] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 1864, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #quality_metric: host=algo-1, epoch=17, train binary_classification_accuracy <score>=0.7455839512711865\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #quality_metric: host=algo-1, epoch=17, train binary_classification_cross_entropy <loss>=0.6089020211817855\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #quality_metric: host=algo-1, epoch=17, train binary_f_1.000 <score>=0.7345560045591131\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495542.544134, \"EndTime\": 1615495544.4123933, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1867.6950931549072, \"count\": 1, \"min\": 1867.6950931549072, \"max\": 1867.6950931549072}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495542.5446618, \"EndTime\": 1615495544.4126575, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2714624.0, \"count\": 1, \"min\": 2714624, \"max\": 2714624}, \"Total Batches Seen\": {\"sum\": 5311.0, \"count\": 1, \"min\": 5311, \"max\": 5311}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=80713.80849774826 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #quality_metric: host=algo-1, epoch=18, batch=0 train binary_classification_accuracy <score>=0.740234375\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #quality_metric: host=algo-1, epoch=18, batch=0 train binary_classification_cross_entropy <loss>=0.6258130073547363\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:44 INFO 139973900457792] #quality_metric: host=algo-1, epoch=18, batch=0 train binary_f_1.000 <score>=0.7981790591805766\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:46.239] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1824, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #quality_metric: host=algo-1, epoch=18, train binary_classification_accuracy <score>=0.7488413665254238\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #quality_metric: host=algo-1, epoch=18, train binary_classification_cross_entropy <loss>=0.6060967984846083\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #quality_metric: host=algo-1, epoch=18, train binary_f_1.000 <score>=0.7381897235929467\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495544.412496, \"EndTime\": 1615495546.239938, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1826.9789218902588, \"count\": 1, \"min\": 1826.9789218902588, \"max\": 1826.9789218902588}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495544.4129257, \"EndTime\": 1615495546.2401867, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2865408.0, \"count\": 1, \"min\": 2865408, \"max\": 2865408}, \"Total Batches Seen\": {\"sum\": 5606.0, \"count\": 1, \"min\": 5606, \"max\": 5606}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=82512.95808425733 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #quality_metric: host=algo-1, epoch=19, batch=0 train binary_classification_accuracy <score>=0.75390625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #quality_metric: host=algo-1, epoch=19, batch=0 train binary_classification_cross_entropy <loss>=0.6221166849136353\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:46 INFO 139973900457792] #quality_metric: host=algo-1, epoch=19, batch=0 train binary_f_1.000 <score>=0.8108108108108109\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:48.080] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 1837, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #quality_metric: host=algo-1, epoch=19, train binary_classification_accuracy <score>=0.7520060911016949\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #quality_metric: host=algo-1, epoch=19, train binary_classification_cross_entropy <loss>=0.6033722841133505\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #quality_metric: host=algo-1, epoch=19, train binary_f_1.000 <score>=0.741609928050606\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495546.240026, \"EndTime\": 1615495548.0808797, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1840.3944969177246, \"count\": 1, \"min\": 1840.3944969177246, \"max\": 1840.3944969177246}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495546.2404525, \"EndTime\": 1615495548.0811312, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3016192.0, \"count\": 1, \"min\": 3016192, \"max\": 3016192}, \"Total Batches Seen\": {\"sum\": 5901.0, \"count\": 1, \"min\": 5901, \"max\": 5901}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81911.54162046114 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #quality_metric: host=algo-1, epoch=20, batch=0 train binary_classification_accuracy <score>=0.755859375\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #quality_metric: host=algo-1, epoch=20, batch=0 train binary_classification_cross_entropy <loss>=0.6185147762298584\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:48 INFO 139973900457792] #quality_metric: host=algo-1, epoch=20, batch=0 train binary_f_1.000 <score>=0.8131539611360239\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:49.940] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 1857, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #quality_metric: host=algo-1, epoch=20, train binary_classification_accuracy <score>=0.7551575741525424\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #quality_metric: host=algo-1, epoch=20, train binary_classification_cross_entropy <loss>=0.6007198741880514\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #quality_metric: host=algo-1, epoch=20, train binary_f_1.000 <score>=0.744977967188696\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495548.0809677, \"EndTime\": 1615495549.9419036, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1860.466718673706, \"count\": 1, \"min\": 1860.466718673706, \"max\": 1860.466718673706}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495548.0814016, \"EndTime\": 1615495549.9422324, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3166976.0, \"count\": 1, \"min\": 3166976, \"max\": 3166976}, \"Total Batches Seen\": {\"sum\": 6196.0, \"count\": 1, \"min\": 6196, \"max\": 6196}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81023.79867409439 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #quality_metric: host=algo-1, epoch=21, batch=0 train binary_classification_accuracy <score>=0.765625\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #quality_metric: host=algo-1, epoch=21, batch=0 train binary_classification_cross_entropy <loss>=0.6150006651878357\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:49 INFO 139973900457792] #quality_metric: host=algo-1, epoch=21, batch=0 train binary_f_1.000 <score>=0.8208955223880597\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:51.783] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1839, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #quality_metric: host=algo-1, epoch=21, train binary_classification_accuracy <score>=0.7583752648305084\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #quality_metric: host=algo-1, epoch=21, train binary_classification_cross_entropy <loss>=0.59813214863761\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #quality_metric: host=algo-1, epoch=21, train binary_f_1.000 <score>=0.7483572024519572\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495549.9419947, \"EndTime\": 1615495551.7843463, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1841.7584896087646, \"count\": 1, \"min\": 1841.7584896087646, \"max\": 1841.7584896087646}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495549.942553, \"EndTime\": 1615495551.7845929, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3317760.0, \"count\": 1, \"min\": 3317760, \"max\": 3317760}, \"Total Batches Seen\": {\"sum\": 6491.0, \"count\": 1, \"min\": 6491, \"max\": 6491}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 23.0, \"count\": 1, \"min\": 23, \"max\": 23}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81850.68052341763 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #quality_metric: host=algo-1, epoch=22, batch=0 train binary_classification_accuracy <score>=0.76953125\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #quality_metric: host=algo-1, epoch=22, batch=0 train binary_classification_cross_entropy <loss>=0.6115681529045105\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:51 INFO 139973900457792] #quality_metric: host=algo-1, epoch=22, batch=0 train binary_f_1.000 <score>=0.8244047619047619\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:53.598] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 1811, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #quality_metric: host=algo-1, epoch=22, train binary_classification_accuracy <score>=0.7614406779661017\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #quality_metric: host=algo-1, epoch=22, train binary_classification_cross_entropy <loss>=0.5956026992555392\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #quality_metric: host=algo-1, epoch=22, train binary_f_1.000 <score>=0.7515548507205406\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495551.784434, \"EndTime\": 1615495553.5993283, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1814.4371509552002, \"count\": 1, \"min\": 1814.4371509552002, \"max\": 1814.4371509552002}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495551.7848582, \"EndTime\": 1615495553.5995858, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3468544.0, \"count\": 1, \"min\": 3468544, \"max\": 3468544}, \"Total Batches Seen\": {\"sum\": 6786.0, \"count\": 1, \"min\": 6786, \"max\": 6786}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=83082.85682178882 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #quality_metric: host=algo-1, epoch=23, batch=0 train binary_classification_accuracy <score>=0.7734375\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #quality_metric: host=algo-1, epoch=23, batch=0 train binary_classification_cross_entropy <loss>=0.6082116365432739\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:53 INFO 139973900457792] #quality_metric: host=algo-1, epoch=23, batch=0 train binary_f_1.000 <score>=0.827893175074184\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:55.419] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 1817, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #quality_metric: host=algo-1, epoch=23, train binary_classification_accuracy <score>=0.7644465042372881\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #quality_metric: host=algo-1, epoch=23, train binary_classification_cross_entropy <loss>=0.5931259500778328\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #quality_metric: host=algo-1, epoch=23, train binary_f_1.000 <score>=0.7547055335695867\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495553.599417, \"EndTime\": 1615495555.420433, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1820.5485343933105, \"count\": 1, \"min\": 1820.5485343933105, \"max\": 1820.5485343933105}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495553.599853, \"EndTime\": 1615495555.4206781, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3619328.0, \"count\": 1, \"min\": 3619328, \"max\": 3619328}, \"Total Batches Seen\": {\"sum\": 7081.0, \"count\": 1, \"min\": 7081, \"max\": 7081}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=82804.97598417939 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #quality_metric: host=algo-1, epoch=24, batch=0 train binary_classification_accuracy <score>=0.77734375\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #quality_metric: host=algo-1, epoch=24, batch=0 train binary_classification_cross_entropy <loss>=0.6049262285232544\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:55 INFO 139973900457792] #quality_metric: host=algo-1, epoch=24, batch=0 train binary_f_1.000 <score>=0.8313609467455622\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:57.263] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1840, \"num_examples\": 295, \"num_bytes\": 9650176}\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, epoch=24, train binary_classification_accuracy <score>=0.7676178495762712\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, epoch=24, train binary_classification_cross_entropy <loss>=0.590697037365477\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, epoch=24, train binary_f_1.000 <score>=0.7581430924112649\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=0.7676178495762712\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, train binary_classification_cross_entropy <loss>=0.590697037365477\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, train binary_f_1.000 <score>=0.7581430924112649\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495555.4205189, \"EndTime\": 1615495557.2642267, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1843.2543277740479, \"count\": 1, \"min\": 1843.2543277740479, \"max\": 1843.2543277740479}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495555.420936, \"EndTime\": 1615495557.2645159, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3770112.0, \"count\": 1, \"min\": 3770112, \"max\": 3770112}, \"Total Batches Seen\": {\"sum\": 7376.0, \"count\": 1, \"min\": 7376, \"max\": 7376}, \"Max Records Seen Between Resets\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Max Batches Seen Between Resets\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 150784.0, \"count\": 1, \"min\": 150784, \"max\": 150784}, \"Number of Batches Since Last Reset\": {\"sum\": 295.0, \"count\": 1, \"min\": 295, \"max\": 295}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #throughput_metric: host=algo-1, train throughput=81781.88224221888 records/second\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 WARNING 139973900457792] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495557.2643285, \"EndTime\": 1615495557.2694557, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 4.511356353759766, \"count\": 1, \"min\": 4.511356353759766, \"max\": 4.511356353759766}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] Saved checkpoint to \"/tmp/tmpj9siz0d4/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:57.317] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 47619, \"num_examples\": 1, \"num_bytes\": 32768}\u001b[0m\n",
      "\u001b[34m[2021-03-11 20:45:57.494] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 176, \"num_examples\": 19, \"num_bytes\": 592512}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495557.3174958, \"EndTime\": 1615495557.4947565, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9258.0, \"count\": 1, \"min\": 9258, \"max\": 9258}, \"Total Batches Seen\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Records Seen Between Resets\": {\"sum\": 9258.0, \"count\": 1, \"min\": 9258, \"max\": 9258}, \"Max Batches Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 9258.0, \"count\": 1, \"min\": 9258, \"max\": 9258}, \"Number of Batches Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #test_score (algo-1) : ('binary_classification_accuracy', 0.6938863685461223)\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #test_score (algo-1) : ('binary_classification_cross_entropy', 0.6316943151128295)\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #test_score (algo-1) : ('binary_f_1.000', 0.6761882998171846)\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, test binary_classification_accuracy <score>=0.6938863685461223\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, test binary_classification_cross_entropy <loss>=0.6316943151128295\u001b[0m\n",
      "\u001b[34m[03/11/2021 20:45:57 INFO 139973900457792] #quality_metric: host=algo-1, test binary_f_1.000 <score>=0.6761882998171846\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1615495557.2695193, \"EndTime\": 1615495557.4960778, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 24.369239807128906, \"count\": 1, \"min\": 24.369239807128906, \"max\": 24.369239807128906}, \"totaltime\": {\"sum\": 47833.65488052368, \"count\": 1, \"min\": 47833.65488052368, \"max\": 47833.65488052368}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-11 20:46:07 Uploading - Uploading generated training model\n",
      "2021-03-11 20:46:07 Completed - Training job completed\n",
      "Training seconds: 110\n",
      "Billable seconds: 110\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "\n",
    "output_prefix = 's3://' + bucket + '/sagemaker-fm/model'\n",
    "instance_type='ml.m4.xlarge'\n",
    "batch_size = 512\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    image_uris.retrieve(\"factorization-machines\", boto3.Session().region_name),\n",
    "    role,\n",
    "    instance_count=1, \n",
    "    instance_type=instance_type,\n",
    "    output_path=output_prefix,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=X_train.shape[1],\n",
    "                       predictor_type='binary_classifier',\n",
    "                       mini_batch_size=batch_size,\n",
    "                       num_factors=128,\n",
    "                       epochs=25,\n",
    "                       clip_gradient=5.0,\n",
    "                       rescale_grad=1.0/batch_size)\n",
    "\n",
    "fm.fit({'train': fm_train_data_path, 'test': fm_test_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code \n",
    "```\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "\n",
    "#output_prefix= 's3://<LabBucketName>/sagemaker-fm/model'\n",
    "\n",
    "output_prefix = 's3://' + bucket + '/sagemaker-fm/model'\n",
    "instance_type='ml.m4.xlarge'\n",
    "batch_size = 512\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    image_uris.retrieve(\"factorization-machines\",boto3.Session().region_name),\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type=instance_type,\n",
    "    output_path=output_prefix,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=X_train.shape[1],\n",
    "                     # predictor_type='regressor',\n",
    "                       predictor_type='binary_classifier',\n",
    "                       mini_batch_size=batch_size,\n",
    "                       num_factors=128,\n",
    "                       epochs=25,\n",
    "                       clip_gradient=5.0,\n",
    "                       rescale_grad=1.0/batch_size\n",
    "                       )\n",
    "\n",
    "fm.fit({'train': fm_train_data_path, 'test': fm_test_data_path})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of this new model. Deploy the model, determine a serializer, and then pass the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import deserializers\n",
    "fm_predictor = fm.deploy(initial_instance_count=1, \n",
    "                         instance_type='ml.m4.xlarge', \n",
    "                         serializer=fm_serializer, \n",
    "                         deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the testing data to the classifier and get all the predictions\n",
    "Y_pred = []\n",
    "for i in range(0, X_test_class.shape[0], 5):\n",
    "    preds = fm_predictor.predict(X_test_class[i:i+5].toarray())['predictions']\n",
    "    p = [Y_pred.append(x['score']) for x in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the results\n",
    "\n",
    "To inspect how well the classifier is doing, calculate and plot a confusion matrix. Use the implementation from **Scikit-Learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3465  994]\n",
      " [1840 2959]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true = Y_test_class.astype(int)\n",
    "predicted = [1 if value > 0.5 else 0 for value in Y_pred]\n",
    "conf_matrix = confusion_matrix(true, predicted)\n",
    "print(conf_matrix)\n",
    "sns.heatmap(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the accuracy of your model?  \n",
    "\n",
    "**Hint**:\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + FP + FN + TN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6938863685461223"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "acc = (tp + tn)/(tp + fp + fn + tn)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How did your model do compared to a naive baseline model of predicting everything as 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    50.536109\n",
       "True     49.463891\n",
       "Name: star_rating, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reduced_df.star_rating > 4).value_counts() / reduced_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# Delete inference endpoint\n",
    "sagemaker.Session().delete_endpoint(fm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining powers with KNN\n",
    "\n",
    "You saw that your classifier model is doing a better job than the regressor model. Now, see if you can repackage it to fit a k-nearest neighbor (KNN) model to predict the *k closest* items to the one a customer likes and then recommend those, instead of predicting the ratings (regressor) or whether a user would like a movie or not (binary classification).\n",
    "\n",
    "Start by downloading the model from Amazon S3. Then, repackage it to fit a KNN model.\n",
    "\n",
    "**Note:** Make sure the kernel you are using is `conda_mxnet_p36` so you can run the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "model_file_name = 'model.tar.gz'\n",
    "model_full_path = f'{fm.output_path}/{fm.latest_training_job.job_name}/output/{model_file_name}'\n",
    "print(f'Model Path: {model_full_path}')\n",
    "\n",
    "# Download FM model \n",
    "os.system('aws s3 cp ' + model_full_path + ' .')\n",
    "\n",
    "# Extract model file for loading to MXNet\n",
    "os.system('tar xzvf ' + model_file_name)\n",
    "os.system('unzip -o model_algo-1')\n",
    "os.system('mv symbol.json model-symbol.json')\n",
    "os.system('mv params model-0000.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract model data to create item and user latent matrixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are going to extract the values that represent each user and item after training a factorization machine. The result of the training is two matrices that, when multiplied together, will represent the target values (zero or one) as closely as possible.\n",
    "\n",
    "In more mathematical terms, factorization machines model output consists of three N-dimensional arrays (ndarrays):\n",
    "\n",
    "    V – a (N x k) matrix, where:\n",
    "        k is the dimension of the latent space\n",
    "        N is the total count of users and items\n",
    "    w – an N-dimensional vector\n",
    "    b – a single number: the bias term\n",
    "\n",
    "To extract these values, which you will use as features, you need to first load the model. Then, extract the values of each of the three matrices and build the `knn_item_matrix` and t`knn_user_matrix` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model data\n",
    "m = mx.module.Module.load('./model', 0, False, label_names=['out_label'])\n",
    "V = m._arg_params['v'].asnumpy()\n",
    "w = m._arg_params['w1_weight'].asnumpy()\n",
    "b = m._arg_params['w0_weight'].asnumpy()\n",
    "\n",
    "nb_users = customers.shape[0]\n",
    "nb_item = products.shape[0]\n",
    "\n",
    "# Item latent matrix - concat(V[i], w[i]).  \n",
    "knn_item_matrix = np.concatenate((V[nb_users:], w[nb_users:]), axis=1)\n",
    "knn_train_label = np.arange(1,nb_item+1)\n",
    "\n",
    "# User latent matrix - concat (V[u], 1) \n",
    "ones = np.ones(nb_users).reshape((nb_users, 1))\n",
    "knn_user_matrix = np.concatenate((V[:nb_users], ones), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building KNN model\n",
    "\n",
    "Now that you have the training data, you can now feed it to a KNN model. As you did before, you need to save the protobuf IO formatted data to Amazon S3, instantiate the model, and set the hyperparameters.\n",
    "\n",
    "Start by setting up the path and the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN train features shape = ', knn_item_matrix.shape)\n",
    "knn_prefix = 'knn'\n",
    "train_key = 'train_knn'\n",
    "knn_output_prefix  = f's3://{bucket}/{knn_prefix}/output'\n",
    "knn_train_data_path = writeDatasetToProtobuf(knn_item_matrix, bucket, \n",
    "                                             knn_prefix, train_key, \n",
    "                                             \"dense\", \n",
    "                                             knn_train_label)\n",
    "print(f'Uploaded KNN train data: {knn_train_data_path}')\n",
    "\n",
    "nb_recommendations = 100\n",
    "\n",
    "# Set up the estimator\n",
    "knn = sagemaker.estimator.Estimator(\n",
    "    image_uris.retrieve(\"knn\",boto3.Session().region_name),\n",
    "    get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will set the hyperparameters. Note that this approach uses the default `index_type` parameter for KNN. It is precise but can be slow for large datasets. In such cases, you may want to use a different `index_type` parameter leading to an approximate, yet faster answer.\n",
    "\n",
    "For more information about index types, see [k-NN Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "knn.set_hyperparameters(feature_dim=knn_item_matrix.shape[1], \n",
    "                        k=nb_recommendations, \n",
    "                        index_metric=\"INNER_PRODUCT\", \n",
    "                        predictor_type='classifier', \n",
    "                        sample_size=200000)\n",
    "\n",
    "\n",
    "knn.fit({'train': knn_train_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model, save it so you can reference it for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_name =  knn.latest_training_job.job_name\n",
    "print(\"created model: \", knn_model_name)\n",
    "\n",
    "# Save the model so that you can reference it in the next step during batch inference\n",
    "sm = boto3.client(service_name='sagemaker')\n",
    "primary_container = {\n",
    "    'Image': knn.image_name,\n",
    "    'ModelDataUrl': knn.model_data,\n",
    "}\n",
    "\n",
    "knn_model = sm.create_model(\n",
    "        ModelName = knn.latest_training_job.job_name,\n",
    "        ExecutionRoleArn = knn.role,\n",
    "        PrimaryContainer = primary_container)\n",
    "print(\"saved the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform\n",
    "\n",
    "To see the predictions your model made, you would have to create inferences and see if they make sense. You could repeat the process as last time and check one user at a time with all possible combinations of items. However, Amazon SageMaker provides a batch transform job that you can use to do inference over the entire dataset. For more information, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "In this section, you will use a batch transform to predict the top 100 recommendations for all the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Upload inference data to S3\n",
    "knn_batch_data_path = writeDatasetToProtobuf(knn_user_matrix,\n",
    "                                             bucket, \n",
    "                                             knn_prefix, \n",
    "                                             train_key, \n",
    "                                             \"dense\")\n",
    "print (\"Batch inference data path: \",knn_batch_data_path)\n",
    "\n",
    "# Initialize the transformer object\n",
    "transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name=\"knn\",\n",
    "    model_name=knn_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    accept=\"application/jsonlines; verbose=true\",\n",
    "    \n",
    ")\n",
    "\n",
    "# Start a transform job\n",
    "transformer.transform(knn_batch_data_path, \n",
    "                      content_type='application/x-recordio-protobuf',\n",
    "                      split_type='RecordIO')\n",
    "transformer.wait()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now free to examine the predictions. Download them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download predictions \n",
    "results_file_name = \"inference_output\"\n",
    "inference_output_file = \"knn/output/train_knn.out\"\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(bucket, inference_output_file, results_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and load it to memory\n",
    "with open(results_file_name) as f:\n",
    "    results = f.readlines() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results contain the 100 nearest neighbor movie IDs with their corresponding distances. See how it looks for user number 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_user_idx = 200\n",
    "u_one_json = json.loads(results[test_user_idx])\n",
    "recommended_movies = [int(movie_id) for movie_id in u_one_json['labels']]\n",
    "distances = [round(distance, 4) for distance in u_one_json['distances']]\n",
    "\n",
    "print(f'Recommended movie Ids for user #{test_user_idx} : {recommended_movies}')\n",
    "\n",
    "print(f'Movie distances for user #{test_user_idx} : {distances}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got the movies closest to user 200's tastes. Now, you can see the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_200 = reduced_df[reduced_df.item.isin(recommended_movies)].product_title.unique()\n",
    "titles_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare them with the favorite movies for user 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.query('user==200 & star_rating == 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do you think these recommendations make sense? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isin(titles_200, titles.tail(100).product_title.unique()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Super bonus question:** Recover the predictions for user 201, and see how they compare with user 200. Are they still correlated? Do you think this approach was an improvement over the first regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the predictions for user 201\n",
    "\n",
    "test_user_idx = 201\n",
    "u_one_json = json.loads(results[test_user_idx])\n",
    "recommended_movies_201 = [int(movie_id) for movie_id in u_one_json['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out recommendations\n",
    "\n",
    "titles_201 = reduced_df[reduced_df.item.isin(recommended_movies_201)].product_title.unique()\n",
    "titles_201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two predictions\n",
    "\n",
    "overlap = np.isin(titles_200, titles_201).sum()\n",
    "print(f'The recommendations for \"user 201\" that are present in \"user 200\" are: {overlap} out of: {len(titles_200)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with user 201 likes\n",
    "\n",
    "reduced_df.query('user==201 & star_rating == 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_idx = 900\n",
    "u_one_json = json.loads(results[test_user_idx])\n",
    "recommended_movies_900 = [int(movie_id) for movie_id in u_one_json['labels']]\n",
    "titles_900 = reduced_df[reduced_df.item.isin(recommended_movies_201)].product_title.unique()\n",
    "overlap_900 = np.isin(titles_200, titles_900).sum()\n",
    "print(f'The recommendations for \"user 900\" that are present in \"user 200\" are: {overlap} out of: {len(titles_200)}')\n",
    "reduced_df.query('user==900 & star_rating == 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of things you can do to improve these models, such as adding features besides rating, trying different feature selection, hyperparameter tuning, and changing the models. The most sophisticated recommendation algorithms are based on deep learning. This can also be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it! You now have a working recommender system that can tell you the top 100 movies for a user. Feel free to optimize and play with the hyperparameters and data to see if you can create an even better recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "In this notebook, you used different techniques to create a recommendations system using only Amazon SageMaker built-in algorithms. You learned how to prepare data in different formats and do feature engineering. You were able to identify problems with your trained models and reframe the problem in different ways to achieve an end result. \n",
    "\n",
    "As you can see now, training a model requires a lot of steps, preparation, and validation. It is not a streamlined process but an iterative one. You can think of this as a virtuous cycle that usually has the following steps:\n",
    "\n",
    "- Define the (business) problem.\n",
    "- Frame the problem as a machine learning problem.\n",
    "- Prepare data and perform freature engineerin.\n",
    "- Train and evaluate the model.\n",
    "- Deploy the model (inference).\n",
    "- Monitor and evaluate.\n",
    "\n",
    "Every step has its own challenges, and each of the steps feeds each other. So it is important to pay attention to the entire pipeline, not only the model training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
